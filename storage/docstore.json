{"docstore/metadata": {"495681bc-d51f-4308-b31a-84caa8468a3b": {"doc_hash": "aeb483799258cdd4d57f92a809a4ffc9e591cbb990295359bd76831b105696e8"}, "a0dee89d-f766-415a-84eb-2ed0212c6e4e": {"doc_hash": "bbda9394ffe051b7927cc549030f8999cf8834715f6fcbfcdeecab576f5e7aee"}, "b6e267d8-1c49-4c47-a166-2566b6ffc3b5": {"doc_hash": "386860316bf1059941015c2b46a9bbbb3bc5b76fa4e22a3e09fba9e22fae9114"}, "77bb7dc3-71c2-4862-9653-1c6a188f9589": {"doc_hash": "006075c3536fd3a930cef786c5df66c31a3e897c69b31edd1c3eaac5c72b5308"}, "5f687132-f180-41f6-81f4-73289528c737": {"doc_hash": "dc234113ad293ce04504ae3f1c6393085b7267043cd136c86ae60c82fb258dc7"}, "4e5849c6-12e9-4373-9da0-5decbb86017e": {"doc_hash": "32bfda686f315f23fc81c312dfdb47bc30a58ea627f43e2d53ebad6e76d4f073"}, "e71b2695-f5dc-4ee1-a004-eb1e2fd0a09c": {"doc_hash": "6db8beee9bd54e5ded014368f6672fd89f632eab4d5545845f120b61ae0446c1"}, "0299c721-d0c8-4bae-a221-b8008e112839": {"doc_hash": "786e9e3b89c5059da5f193a133b0ad4a1f3aa355901ea6d9fab581d39d903e44"}, "ee28887d-94b6-4892-b0f8-0d3ce82210ab": {"doc_hash": "20c59a918bb7d074524b95b765f367d21589e021549777692717eb73d95130d4"}, "42fd54e6-5473-4cbf-9d08-90c03c85bbde": {"doc_hash": "bdb45cbb112bbbe88430b47a64fa5b6f018501a22921298dc53250af5d99957a"}, "a275cc4e-a022-498c-a869-6013536c367f": {"doc_hash": "5daf3ce4c631cf2ebafde77fe5af84b555403231a90f29080afbf2cffe82f338"}, "d5dfac96-c94e-48b7-927d-ace75146706f": {"doc_hash": "37f9ef85f40e3e94d2c1e0a503813b6aca4f6f55cb90acf6875353a0cebb8015"}, "8d94c55a-8566-4f87-a584-9786cc35f753": {"doc_hash": "902a0f1e310ffbea9ffca9f8ce1375ee0fae7163350fddca6b339d8e2652832f"}, "f40f0e73-aa49-467f-b26e-1b77ae3a1f79": {"doc_hash": "fa7044440a5edfdbce9feb759b9258367e5a52745315fca2839ccb5cba4cc417"}, "2ac0cc0d-5faa-4e16-82a4-1f3a303d7006": {"doc_hash": "96bb9acf25eb95830ec589f00482609de5e4ba345d8961112381ddf9914f5fef"}, "a31fcd72-9352-4e2a-a6ff-4a35fa85a8bf": {"doc_hash": "aece9834d4ed02f46c3f57c41581e8449934d3f831e9903f4a0fc4fdb19f478c"}, "a9025ae8-c380-403f-8f01-abf432feb81e": {"doc_hash": "0cf85f647dee1d46a8aca5abebbef8042fd8ad29d719590b63d1dc897df68155"}, "c48cb58e-c7c4-45aa-8210-a877662d6ec5": {"doc_hash": "3ca3237fe04a791e9f34cea84bcfee420550efa82ad7c8dbdd7f4d9e9c76706b"}, "e2f11d09-4e6e-4ee8-aa84-500728571ef9": {"doc_hash": "201b58a8163ff21b98381363a7b02e479dd75d3ed16898f4613a6e9272b638ca"}, "f735b33d-be49-47f2-a8e4-614d2963e5d5": {"doc_hash": "85ed81d1fd6a7b21a07278b37c1c33e678f97c6651f55a61c3fe49de85503cf6"}, "b37bbca1-ad20-4b10-a125-5703360ec665": {"doc_hash": "8113e8b11fdfe672b0245a7d291c946d8f43a63838b7d2f09b01f7d54dbe3ed7"}, "f6bd37e8-19bd-4616-981e-541d875446b0": {"doc_hash": "d7ba8a07ee74012dc03cd0f6d6f116bad3e2ce117d4f55a704121c9f0ada835c"}, "d30725b2-8917-4b80-aa26-b6ab1d5d4328": {"doc_hash": "1ed2c91480ca076f15043ad79b4b9f86b475f3065957e611509177b95958ee16"}, "9366bf78-585a-487f-919f-aaa1e1c8a2d7": {"doc_hash": "0226df85e51cffd34e115441f77690a3bd575f984f9c0af13e2a561ad4d7049c"}, "c85e4e9f-d83b-429e-925d-502f1e8647e4": {"doc_hash": "c87d8f6d5162ca151bd28a4a4d4694a0df0de1e1de28b80a5f36b0bd65292750"}, "89a4565d-5b89-4b6a-9758-40eb6b17b1bb": {"doc_hash": "48c1123e96681284cfa4e202cea2ae4b80e11763a7ec496a5960549025f2ee0c"}, "8ecdbeb9-8a12-4d93-81eb-d9f4146f377e": {"doc_hash": "12eecc5dbf70bb045c9f3aa6356f9eae3448d57d7c5127a971070180eee17ba5"}, "313c1bb9-a842-416a-b74f-d7ba5963f8b9": {"doc_hash": "742d84c4ba1600bc5b6118bf7fd04e312a9e3d18f8765c18b10a0772f688dc62"}, "fc92ecc4-86aa-46fd-938a-eb82b83e7c8b": {"doc_hash": "4e57516d8f90affd0c4d2929306120c712b3cd94a0351ccf09955402bd0de268"}, "d67511a4-54ba-44c6-a886-b59f561c5ae5": {"doc_hash": "597d9547c6088bea14b48d158b5b34b516a73f849fa9bdcf77d90c4cae86c021"}, "f879b90f-dfbd-4773-9ece-db88ae9d007f": {"doc_hash": "b82c8f3d329ad7fcdcd9aa9278bed2613044583774863e83f9ee91399bf2d018"}, "098cdf05-b928-44fb-8c61-ccc493ccce90": {"doc_hash": "c15b9eff26ff2cc30108ec1aaebfd37729273251578dadf1af18ae73c3b94c6e"}, "98b7239c-c3f6-474c-a5b1-86bdb0f9b7e5": {"doc_hash": "7f0787ce690edfe3fa1dc75a11f796aa01a67c5e4889f8438057327a8adadf48"}, "95c5161b-01a6-4cd3-a36e-adac2180cd0b": {"doc_hash": "c5e15ba46dc63c7e7b745af2553d2757fd34a7eaeadd04485caa77346cc94667"}, "9e103825-8811-48ec-8070-e9b5f6ce841d": {"doc_hash": "a9f22e082e8c226b38909d21ae8d22642aeca1a025fdc5aac026fd1f4cc453ea"}, "3c690256-4434-442d-bb66-307798698755": {"doc_hash": "8a456588e89bc0b20a9acd4b4dd268512bdd711299d90414ef65c0caeffc854a", "ref_doc_id": "495681bc-d51f-4308-b31a-84caa8468a3b"}, "06930175-2152-4a27-b699-a0244fcb66b7": {"doc_hash": "30575f56fe2d66e29218caa36f9e1682290413064e89175c544a7d063dff7673", "ref_doc_id": "495681bc-d51f-4308-b31a-84caa8468a3b"}, "3a6394d7-c53a-482f-a085-fe633c55c805": {"doc_hash": "bbda9394ffe051b7927cc549030f8999cf8834715f6fcbfcdeecab576f5e7aee", "ref_doc_id": "a0dee89d-f766-415a-84eb-2ed0212c6e4e"}, "6dcb0487-8c7f-4f26-9ca2-aa72ddd9bba1": {"doc_hash": "70501427bcec011972d38dc9c99d525d936d427ef4fcdb063dc6c4ea9efaada9", "ref_doc_id": "b6e267d8-1c49-4c47-a166-2566b6ffc3b5"}, "9f9cdd59-7a12-46d5-9215-e6a61d3c87c5": {"doc_hash": "6d4ae42411c15765fa4518aa30affd431297a22a2d220934005ce72f2aa3820e", "ref_doc_id": "b6e267d8-1c49-4c47-a166-2566b6ffc3b5"}, "4022fbee-91c7-4511-b72c-a311a8b732f3": {"doc_hash": "ca4cd2c791ab1c64381dbc4a833b01bed04d35303d0482f3de4714e646f12374", "ref_doc_id": "b6e267d8-1c49-4c47-a166-2566b6ffc3b5"}, "c3633805-5a5f-4762-b6a0-3e6c58f1bf94": {"doc_hash": "1b7529e03949e19784f669aeadb97c6e11feafcc6e6c452eb6426015e6305f88", "ref_doc_id": "b6e267d8-1c49-4c47-a166-2566b6ffc3b5"}, "76847fe2-68b2-4d53-8dbb-72eed750f1dd": {"doc_hash": "add236b3be40d49fd7991d1ed2103e7556e2b699f592bcb62bbb8cc7e6952974", "ref_doc_id": "b6e267d8-1c49-4c47-a166-2566b6ffc3b5"}, "64d15b93-d8a9-4b51-8302-e377fa135abe": {"doc_hash": "0babe36891331373bb928f7649903f3319d9253f0331947b6e08ad11da9f744d", "ref_doc_id": "77bb7dc3-71c2-4862-9653-1c6a188f9589"}, "82ea3b5a-aaf4-4b6f-971b-3b683badc115": {"doc_hash": "31d9540ee437793d0451cebb16b506faf45904c2802eea567f4d1b53cddef7a1", "ref_doc_id": "77bb7dc3-71c2-4862-9653-1c6a188f9589"}, "21812bc7-cdc8-428f-96be-c54e3263dc38": {"doc_hash": "46679db05e95e7fbee8fc26a5e73ea9f364782b4b74810945a726a956988a848", "ref_doc_id": "5f687132-f180-41f6-81f4-73289528c737"}, "db73b87d-b7ed-44a4-afca-4738c55a929d": {"doc_hash": "316bd50ece8ae87a5379a9ffd1f16c4073f688a2fc58be16c06e195cb68107d6", "ref_doc_id": "5f687132-f180-41f6-81f4-73289528c737"}, "760f5448-22ac-477b-ad23-f88ed331c0f3": {"doc_hash": "8b0f47018fb8fbf5a87f55b435d0b6c0dd2601c6d5fe8e3a347c433403010f0a", "ref_doc_id": "4e5849c6-12e9-4373-9da0-5decbb86017e"}, "0b0ebff4-a6fb-4d3f-a597-2fa254cc5360": {"doc_hash": "02f209c42ad91739fd9006bbf3debc4a94d926ce8ad36d3461dfdbe1edace1b7", "ref_doc_id": "4e5849c6-12e9-4373-9da0-5decbb86017e"}, "0a581623-32a7-4b0a-af17-10401dae3dc7": {"doc_hash": "81331ccb769051548c163e45ec9c4ffdb3db44447bbfa4b8faaca8029b7118ff", "ref_doc_id": "e71b2695-f5dc-4ee1-a004-eb1e2fd0a09c"}, "a31faf5a-76a0-48dc-86b1-1dada8caf085": {"doc_hash": "12c3074bb65ada167f4004d064b1b986600ce5565bc591e0a07ab14ba8185aba", "ref_doc_id": "e71b2695-f5dc-4ee1-a004-eb1e2fd0a09c"}, "71017e2f-415d-4348-b708-8b0e34a8dfe5": {"doc_hash": "1a3e193428f95018ae32c6fdfa84b42e3c42224899c1e029a9a156e5171aa609", "ref_doc_id": "0299c721-d0c8-4bae-a221-b8008e112839"}, "91e2a0ab-3657-4260-8cbc-34b0e827b9fa": {"doc_hash": "98ac09094fac5d6bc7a9480312f99076d66faf1c866af2c9ec5cd092e8abcd90", "ref_doc_id": "0299c721-d0c8-4bae-a221-b8008e112839"}, "dc71cc6f-3115-4644-bfe5-acf79adb2a26": {"doc_hash": "2577bcca99473d9ab7e72ccb9520ff87531100ed0a1c996c7105f9845afce0eb", "ref_doc_id": "ee28887d-94b6-4892-b0f8-0d3ce82210ab"}, "b3ba7b87-ccaa-4e9e-93e4-7a56ce10ebcf": {"doc_hash": "80b61c5215e56ab951ce7c2228162fe94c2850d729bd5244afb8d2f85b586232", "ref_doc_id": "ee28887d-94b6-4892-b0f8-0d3ce82210ab"}, "f1312f1e-7f6e-4e0e-be92-f3c3ccb7a014": {"doc_hash": "49ad5f31978c87fa9818863470c587b1c06d6c3c6f637530857accda56706c7d", "ref_doc_id": "42fd54e6-5473-4cbf-9d08-90c03c85bbde"}, "3fdff81a-7742-4405-a889-a3b820277638": {"doc_hash": "301fb6ab50bfd829296274badadc9f939977625377efd9f619513f7a6f62f62c", "ref_doc_id": "42fd54e6-5473-4cbf-9d08-90c03c85bbde"}, "f54ff78d-a4a3-4460-a49a-41f88a058079": {"doc_hash": "1ad06ffac69d235887d4d2a2d3c36dbece10f97616e44b4aa8a777f3fdf378ed", "ref_doc_id": "a275cc4e-a022-498c-a869-6013536c367f"}, "003a9261-5643-4655-afab-e25cb6260094": {"doc_hash": "588e3f335861459488e83a209ca012c833dc082ff10132891b37f9e254c0017c", "ref_doc_id": "a275cc4e-a022-498c-a869-6013536c367f"}, "3948973c-04a3-4443-a96d-44b243f042e1": {"doc_hash": "846249fe626e835617a69ed2b44e299cce0e6012ee8508fe1369f35e842407fe", "ref_doc_id": "d5dfac96-c94e-48b7-927d-ace75146706f"}, "48de820b-9318-4dcc-91db-f0a6c9f8847b": {"doc_hash": "c9e5b2d3de02494adbab81a34b3c2613ce85f1572998fc99bc18477027b08f8a", "ref_doc_id": "d5dfac96-c94e-48b7-927d-ace75146706f"}, "4304dee3-e26d-4278-a690-a35b8e99d5d3": {"doc_hash": "902a0f1e310ffbea9ffca9f8ce1375ee0fae7163350fddca6b339d8e2652832f", "ref_doc_id": "8d94c55a-8566-4f87-a584-9786cc35f753"}, "6f0fe2cc-ca36-493b-bd8e-ea9b218518bd": {"doc_hash": "fa7044440a5edfdbce9feb759b9258367e5a52745315fca2839ccb5cba4cc417", "ref_doc_id": "f40f0e73-aa49-467f-b26e-1b77ae3a1f79"}, "d0581db3-4277-4801-920d-47f32fff28f7": {"doc_hash": "96bb9acf25eb95830ec589f00482609de5e4ba345d8961112381ddf9914f5fef", "ref_doc_id": "2ac0cc0d-5faa-4e16-82a4-1f3a303d7006"}, "bd207ab8-21a7-4063-b9f1-7b3e98043d33": {"doc_hash": "aece9834d4ed02f46c3f57c41581e8449934d3f831e9903f4a0fc4fdb19f478c", "ref_doc_id": "a31fcd72-9352-4e2a-a6ff-4a35fa85a8bf"}, "300cb555-aab8-40dc-b8f6-c467e2e905f9": {"doc_hash": "0cf85f647dee1d46a8aca5abebbef8042fd8ad29d719590b63d1dc897df68155", "ref_doc_id": "a9025ae8-c380-403f-8f01-abf432feb81e"}, "b11f282c-5ead-4150-b193-9ce037de8ff3": {"doc_hash": "3ca3237fe04a791e9f34cea84bcfee420550efa82ad7c8dbdd7f4d9e9c76706b", "ref_doc_id": "c48cb58e-c7c4-45aa-8210-a877662d6ec5"}, "6f00e998-e4a7-4a34-978e-421358819c3a": {"doc_hash": "201b58a8163ff21b98381363a7b02e479dd75d3ed16898f4613a6e9272b638ca", "ref_doc_id": "e2f11d09-4e6e-4ee8-aa84-500728571ef9"}, "518693d7-8057-4a6c-9771-e1a97399f2da": {"doc_hash": "85ed81d1fd6a7b21a07278b37c1c33e678f97c6651f55a61c3fe49de85503cf6", "ref_doc_id": "f735b33d-be49-47f2-a8e4-614d2963e5d5"}, "4dba1649-ec15-489e-9214-d51da906145e": {"doc_hash": "8113e8b11fdfe672b0245a7d291c946d8f43a63838b7d2f09b01f7d54dbe3ed7", "ref_doc_id": "b37bbca1-ad20-4b10-a125-5703360ec665"}, "93d8512f-5257-452f-bce9-5eea5f8c894d": {"doc_hash": "d7ba8a07ee74012dc03cd0f6d6f116bad3e2ce117d4f55a704121c9f0ada835c", "ref_doc_id": "f6bd37e8-19bd-4616-981e-541d875446b0"}, "3b08562d-bf8e-4707-b3aa-4a677729d21f": {"doc_hash": "1ed2c91480ca076f15043ad79b4b9f86b475f3065957e611509177b95958ee16", "ref_doc_id": "d30725b2-8917-4b80-aa26-b6ab1d5d4328"}, "63819359-9937-491d-a1b3-54cdae85df43": {"doc_hash": "0226df85e51cffd34e115441f77690a3bd575f984f9c0af13e2a561ad4d7049c", "ref_doc_id": "9366bf78-585a-487f-919f-aaa1e1c8a2d7"}, "e6ae1976-9d75-4bad-aa02-3dffbb741229": {"doc_hash": "c87d8f6d5162ca151bd28a4a4d4694a0df0de1e1de28b80a5f36b0bd65292750", "ref_doc_id": "c85e4e9f-d83b-429e-925d-502f1e8647e4"}, "6157bec3-85a2-482b-9014-f4e118e443fa": {"doc_hash": "48c1123e96681284cfa4e202cea2ae4b80e11763a7ec496a5960549025f2ee0c", "ref_doc_id": "89a4565d-5b89-4b6a-9758-40eb6b17b1bb"}, "f1560dd1-dbe5-4104-b8b7-c732b7f904fe": {"doc_hash": "a06e3c47e2cb7ae0665c691837e93512c8d93b419150ce52cbb3f1dd675513e0", "ref_doc_id": "8ecdbeb9-8a12-4d93-81eb-d9f4146f377e"}, "1f4f2a6d-b42d-4ebc-99a0-c8aa18dbb9b3": {"doc_hash": "d5d40c64d9464fee69c83a7bbe792eeb7441e23a1eb1c9b8331140a90d37d990", "ref_doc_id": "8ecdbeb9-8a12-4d93-81eb-d9f4146f377e"}, "7952aaaf-cee5-45d1-9f43-5f2de505d4e0": {"doc_hash": "742d84c4ba1600bc5b6118bf7fd04e312a9e3d18f8765c18b10a0772f688dc62", "ref_doc_id": "313c1bb9-a842-416a-b74f-d7ba5963f8b9"}, "e0c8dcbc-1938-462e-b499-ffc4e59f452d": {"doc_hash": "4e57516d8f90affd0c4d2929306120c712b3cd94a0351ccf09955402bd0de268", "ref_doc_id": "fc92ecc4-86aa-46fd-938a-eb82b83e7c8b"}, "2ee954f8-f718-482d-9b12-1e67742f741b": {"doc_hash": "597d9547c6088bea14b48d158b5b34b516a73f849fa9bdcf77d90c4cae86c021", "ref_doc_id": "d67511a4-54ba-44c6-a886-b59f561c5ae5"}, "2433c747-6d6b-4d2f-9410-8fd181b41c7a": {"doc_hash": "b828b25d4cb37cfeb7c8e9234a48dcefb6d5b43641585504506bf6ae1b55dfb6", "ref_doc_id": "f879b90f-dfbd-4773-9ece-db88ae9d007f"}, "6cf5fbd3-9c78-4bf6-94e9-c74b58ff9a19": {"doc_hash": "199ffcce2100212ba73cfee38fdee9493ea06edd8f8c611958cd550767506b53", "ref_doc_id": "f879b90f-dfbd-4773-9ece-db88ae9d007f"}, "4611a36c-1649-4040-b41f-3185f544ee81": {"doc_hash": "c15b9eff26ff2cc30108ec1aaebfd37729273251578dadf1af18ae73c3b94c6e", "ref_doc_id": "098cdf05-b928-44fb-8c61-ccc493ccce90"}, "c75190e4-1b06-4eb0-a4b4-92da6269effb": {"doc_hash": "337731470a27b7d240913210b5f11c53e952994c0009ef71479288396cfa016a", "ref_doc_id": "98b7239c-c3f6-474c-a5b1-86bdb0f9b7e5"}, "22864b2f-6bba-4aa3-9dcf-45d5c130efc9": {"doc_hash": "953d4e4e4dfd4b2e2e7a3d7c8343df93470a13d9e0a35704f64ed2bfe9580fd1", "ref_doc_id": "98b7239c-c3f6-474c-a5b1-86bdb0f9b7e5"}, "09998943-fff4-47ff-a5ac-f8d80ab86268": {"doc_hash": "c5e15ba46dc63c7e7b745af2553d2757fd34a7eaeadd04485caa77346cc94667", "ref_doc_id": "95c5161b-01a6-4cd3-a36e-adac2180cd0b"}, "e0d654c5-7220-45b7-b66f-5ffd6a4ef596": {"doc_hash": "a9f22e082e8c226b38909d21ae8d22642aeca1a025fdc5aac026fd1f4cc453ea", "ref_doc_id": "9e103825-8811-48ec-8070-e9b5f6ce841d"}}, "docstore/data": {"3c690256-4434-442d-bb66-307798698755": {"__data__": {"id_": "3c690256-4434-442d-bb66-307798698755", "embedding": null, "metadata": {"page_label": "1", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "495681bc-d51f-4308-b31a-84caa8468a3b", "node_type": "4", "metadata": {"page_label": "1", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "aeb483799258cdd4d57f92a809a4ffc9e591cbb990295359bd76831b105696e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "06930175-2152-4a27-b699-a0244fcb66b7", "node_type": "1", "metadata": {}, "hash": "fb813998109b8e2933b29a5880c013fdf7c13b9f53aefdbe42e9cc4c017cfd39", "class_name": "RelatedNodeInfo"}}, "text": "DFA-RAG: Conversational Semantic Router for Large Language Model\nwith Definite Finite Automaton\nYiyou Sun1 2Junjie Hu1Wei Cheng2Haifeng Chen2\nAbstract\nThis paper introduces the retrieval-augmented\nlarge language model with Definite Finite Au-\ntomaton (DFA-RAG), a novel framework de-\nsigned to enhance the capabilities of conversa-\ntional agents using large language models (LLMs).\nTraditional LLMs face challenges in generating\nregulated and compliant responses in special sce-\nnarios with predetermined response guidelines,\nlike emotional support and customer service. Our\nframework addresses these challenges by embed-\nding a Definite Finite Automaton (DFA), learned\nfrom training dialogues, within the LLM. This\nstructured approach acts as a semantic router\nwhich enables the LLM to adhere to a determin-\nistic response pathway. The routing is achieved\nby the retrieval-augmentation generation (RAG)\nstrategy, which carefully selects dialogue exam-\nples aligned with the current conversational con-\ntext. The advantages of DFA-RAG include an\ninterpretable structure through human-readable\nDFA, context-aware retrieval for responses in con-\nversations, and plug-and-play compatibility with\nexisting LLMs. Extensive benchmarks validate\nDFA-RAG\u2019s effectiveness, indicating its potential\nas a valuable contribution to the conversational\nagent.\n1. Introduction\nRecent advancements in machine learning, particularly in\nlarge language models (LLMs), have provided more pos-\nsibilities in various fields. Their applications range from\ndocument completion to chatbots (Ouyang et al., 2022; Ope-\nnAI, 2023). These conversational agents with LLMs are\n*Equal contribution1Department of Computer Science, Uni-\nversity of Wisconsin2NEC Laboratories America, inc., Princeton,\nUSA. Correspondence to: Yiyou Sun <sunyiyou@nec-labs.com >,\nHaifeng Chen <haifeng@nec-labs.com >.\nProceedings of the 41stInternational Conference on Machine\nLearning , Vienna, Austria. PMLR 235, 2024. Copyright 2024 by\nthe author(s).remarkably adaptable and have proven effective in diverse\nsettings including customer service, marketing, education,\nand healthcare (Wang et al., 2023b). Compared to tradi-\ntional, rule-based dialogue systems (Abdul-Kader & Woods,\n2015; Hussain et al., 2019; Adamopoulou & Moussiades,\n2020), LLMs offer greater efficiency, scalability, and dy-\nnamism (Medeiros et al., 2019). However, their application\nin practical scenarios often necessitates adherence to spe-\ncific workflows or policies. For instance, an Emotional\nSupport Chatbot (Medeiros et al., 2019) must tailor its re-\nsponses to various stress conditions, while customer service\nbots typically follow predetermined response guidelines.\nIn these scenarios, popular LLMs like GPT-3.5 or GPT-4\nmay generate inappropriate or misleading content (Wang\net al., 2023a) without specialized optimization. Fine-tuning\nthese models, while effective, is not always viable due\nto open-source requirements, the need for intricate de-\nsign (Ding et al., 2023), and substantial data to mitigate over-\nfitting risks (Selvi, 2023). An alternative method, Retrieval-\nAugmented Generation (RAG) (Lewis et al., 2020), which\nreferences an additional knowledge base in generating a\nresponse, has been explored to address these issues. Since\nthe generation quality of RAG is known to be sensitive to\nthe sample selection (Gao et al., 2023), it is therefore criti-\ncal to design an effective selection strategy that ensures the\nretrieval of the most relevant and contextually appropriate\nsamples. This poses unique challenges to apply RAG in\nconversational scenarios, as one must identify partial con-\nversational flows within historical training dialogues that\nclosely match the current conversation context.\nTargeting these challenges, we formally introduce the\nnew framework, DFA-based retrieval-augmented genera-\ntion (dubbed DFA-RAG ). This framework assumes that\na specific workflow is embedded within the training data\ndialogues, which we model using a Definite Finite Au-\ntomaton (as illustrated in Figure 1).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4050, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "06930175-2152-4a27-b699-a0244fcb66b7": {"__data__": {"id_": "06930175-2152-4a27-b699-a0244fcb66b7", "embedding": null, "metadata": {"page_label": "1", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "495681bc-d51f-4308-b31a-84caa8468a3b", "node_type": "4", "metadata": {"page_label": "1", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "aeb483799258cdd4d57f92a809a4ffc9e591cbb990295359bd76831b105696e8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c690256-4434-442d-bb66-307798698755", "node_type": "1", "metadata": {"page_label": "1", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "8a456588e89bc0b20a9acd4b4dd268512bdd711299d90414ef65c0caeffc854a", "class_name": "RelatedNodeInfo"}}, "text": "Acting as a Semantic\nRouter (Horsey, 2024) like a decision-making layer, DFA-\nRAG routes conversations through a predefined trajectory.\nThis ensures that the LLM adheres to the workflow encoded\nin the DFA. Specifically, each conversational utterance cor-\nresponds to a particular DFA state, where each state encapsu-\nlates responses from similar historical contexts. By utilizing\nthese historical examples as retrieved samples, DFA-RAG\n1arXiv:2402.04411v2  [cs.CL]  3 Jun 2024", "mimetype": "text/plain", "start_char_idx": 4051, "end_char_idx": 4527, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3a6394d7-c53a-482f-a085-fe633c55c805": {"__data__": {"id_": "3a6394d7-c53a-482f-a085-fe633c55c805", "embedding": null, "metadata": {"page_label": "2", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a0dee89d-f766-415a-84eb-2ed0212c6e4e", "node_type": "4", "metadata": {"page_label": "2", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "bbda9394ffe051b7927cc549030f8999cf8834715f6fcbfcdeecab576f5e7aee", "class_name": "RelatedNodeInfo"}}, "text": "DFA-RAG: Conversational Semantic Router for Large Language Model with Definite Finite Automaton\n#N\nHow can I help with you?My iPhone battery drain fast.\nWhat is your system version?\nIt is IOS 11.1.\nFollow the steps here to check the usage summary. \nThanks. Will be back soon.\nPlease rate my service here.STARTBatteryIssueHelp\u2026.IOS10.0IOS11.1ThankNoteENDING\n\u2026.\u2026.\nHere is the ongoing conversation:System: How can I help with you?User: My iPhone battery drain fast.Please give response as a helpful customer service agent.  Relevant Examples: \nStored Dialogue ID: [#1, #3, #17, \u2026]\n<CONTENT IN DIALOGUE #1>\u2026\u2026\n<CONTENT IN DIALOGUE #3>PROMPT\u2026.What is your system version?OUTPUTLearn\n(a) Dialogue Training Samples (b) Demo of De\ufb01nite Finite Automaton(c) DFA-based In-Context Learning(d) Demo of Conversations\n#3\n#2\n#1\nFigure 1. Illustration of the DFA-RAG Framework. (a) shows the training set with dialogues. (b) demonstrates the Definite Finite\nAutomaton (DFA) which represents the workflow learned from the dialogues. Blue and green dots represent the states of the user and\nsystem respectively. The states are transited by keywords in conversations. (c) outlines the DFA-based In-Context Learning process,\nwhere the LLM is guided by the DFA to provide contextually relevant responses. (d) showcases sample conversations between a user and\nthe LLM.\neffectively guides the LLM to follow a deterministic DFA-\nbased response pathway. The DFA-RAG framework offers\nseveral compelling advantages:\n1.Interpretable Structure : Unlike RAG (Retrieval-\nAugmented Generation) (Lewis et al., 2020) which fuses\ninformation on uninterpretable embedding spaces, our\napproach ensures responses are generated by human-\nreadable structures (DFA), enhancing the trustworthiness\nof LLM-based chatbots.\n2.Context-Aware Retrieval : Unlike most retrieval-based\nstrategies that treat dialogues as undivided wholes, our\napproach divides each dialogue into manageable seg-\nments. This granular approach allows for more precise\nand contextually relevant information retrieval, enhanc-\ning the chatbot\u2019s ability to respond accurately and appro-\npriately to various conversational nuances.\n3.Plug-and-Play Compatibility : The DFA, derived from\ntraining data, is designed for easy integration with any\npre-trained LLM. This flexibility allows for widespread\napplication across different LLM platforms, making\nDFA-RAG a versatile tool in conversational AI.\n4.Strong Empirical Performance : In domain-specific\nconversations, DFA-RAG has shown superior perfor-\nmance, as demonstrated by extensive evaluation bench-\nmarks. This empirical evidence underscores the effec-\ntiveness of the proposed framework in delivering high-\nquality, domain-relevant responses.2. Problem Setup\nIn our setting, we assume access to a sample set of dialogues\nin the application domain of interest, such as customer ser-\nvice or emotional support. This dataset may be pre-collected\nthrough interactions between a human agent and a customer\nor patient. We now formally describe the data setup.\nData Setup. We consider the training set with Ndialogues\nDtr={xi}N\ni=1. Each dialogue is a sequence of utterances,\ndenoted as xi= (z1, z2, . . . , z li), where lirepresents the\nlength of the i-th dialogue. We assume the utterances alter-\nnate between the agent and the user.\nGoal. At the inference stage, the LLM-based agent gener-\nates the next utterance based on the context of an incomplete\ndialogue \u00afx= (z1, z2, . . . , z j):\n\u02c6zj+1\u223cP(\u02c6z|\u00afx,Dtr).\nThe objective is for the output \u02c6zj+1to closely match the\nhuman agent\u2019s response zj+1.\n3. Methodology\nHuman agents often provide responses that involve navigat-\ning a pre-determined landscape of options, heuristics, and\npotential outcomes. This process can be conceptualized as\nexploring a special branch of a network of interconnected\npaths and junctions. In contrast, current LLMs like GPT-4\nlack the intrinsic ability to mimic this human-like pipeline-\nguided behavior in response generation. This limitation\n2", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3996, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6dcb0487-8c7f-4f26-9ca2-aa72ddd9bba1": {"__data__": {"id_": "6dcb0487-8c7f-4f26-9ca2-aa72ddd9bba1", "embedding": null, "metadata": {"page_label": "3", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b6e267d8-1c49-4c47-a166-2566b6ffc3b5", "node_type": "4", "metadata": {"page_label": "3", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "386860316bf1059941015c2b46a9bbbb3bc5b76fa4e22a3e09fba9e22fae9114", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f9cdd59-7a12-46d5-9215-e6a61d3c87c5", "node_type": "1", "metadata": {}, "hash": "abc800427c7455094b53645e1e4fb7da6b66be930376b96d4e2181c9058ccd35", "class_name": "RelatedNodeInfo"}}, "text": "DFA-RAG: Conversational Semantic Router for Large Language Model with Definite Finite Automaton\nbecomes particularly pronounced in scenarios where adher-\nence to specific workflows is paramount, such as in customer\nservice or healthcare settings. In these cases, traditional\nLLMs might generate responses that are misaligned (Wang\net al., 2023a) with the desired conversational trajectory or\ndecision pathway.\nTo address this gap, our methodology with DFA-RAG em-\nbeds a structured pipeline within the operational paradigm\nof the LLM. By integrating a DFA, we propose to direct\nthe LLM\u2019s response generation process, aligning it more\nclosely with the decision paths of typical human agents. In\nSection 3.1, we introduce how we can model the conversa-\ntions as a DFA. Later in Section 3.2, we delve into details\nof constructing the DFA from training dialogue datasets.\nSubsequently, in Section 3.3, we illustrate the mechanism\nby which the learned DFA guides the LLM.\n3.1. Modeling Conversations with DFA\n3.1.1. P RELIMINARIES OF DFA\nAutomata has a long history of study in theoretical computer\nscience, linguistics, and other related fields (Minsky, 1956;\nKleene et al., 1956). A deterministic finite automaton (DFA)\ncan be specified as a tuple (Q,\u03a3, \u03b4, q 0, F), where:\n\u2022Qis a finite set of states\n\u2022\u03a3is a finite input alphabet\n\u2022\u03b4:Q\u00d7\u03a3\u2192Qis the transition function\n\u2022q0\u2208Qis the start state\n\u2022F\u2286Qis the set of accept states\nThe DFA processes a string of symbols from \u03a3and changes\nits state according to the transition function \u03b4. The string is\naccepted by the DFA if the automaton is in one of the accept\nstates in Fafter processing all symbols.\n<latexit sha1_base64=\"Y9dozFO2T98BvnLl/Vrpps2U824=\">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8eK9gPaUDbbTbt0s4m7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSKQw6LrfTmFldW19o7hZ2tre2d0r7x80TZxqxhsslrFuB9RwKRRvoEDJ24nmNAokbwWjm6nfeuLaiFg94DjhfkQHSoSCUbTS/WPP7ZUrbtWdgSwTLycVyFHvlb+6/ZilEVfIJDWm47kJ+hnVKJjkk1I3NTyhbEQHvGOpohE3fjY7dUJOrNInYaxtKSQz9fdERiNjxlFgOyOKQ7PoTcX/vE6K4ZWfCZWkyBWbLwpTSTAm079JX2jOUI4toUwLeythQ6opQ5tOyYbgLb68TJpnVe+ien53Xqld53EU4QiO4RQ8uIQa3EIdGsBgAM/wCm+OdF6cd+dj3lpw8plD+APn8wcCcI2h</latexit>q0<latexit", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2156, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9f9cdd59-7a12-46d5-9215-e6a61d3c87c5": {"__data__": {"id_": "9f9cdd59-7a12-46d5-9215-e6a61d3c87c5", "embedding": null, "metadata": {"page_label": "3", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b6e267d8-1c49-4c47-a166-2566b6ffc3b5", "node_type": "4", "metadata": {"page_label": "3", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "386860316bf1059941015c2b46a9bbbb3bc5b76fa4e22a3e09fba9e22fae9114", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6dcb0487-8c7f-4f26-9ca2-aa72ddd9bba1", "node_type": "1", "metadata": {"page_label": "3", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "70501427bcec011972d38dc9c99d525d936d427ef4fcdb063dc6c4ea9efaada9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4022fbee-91c7-4511-b72c-a311a8b732f3", "node_type": "1", "metadata": {}, "hash": "81e5c3f09543be164236778bb42386a257d63dfbcc891d9395915d3ac2bee1cc", "class_name": "RelatedNodeInfo"}}, "text": "sha1_base64=\"hQrRdnmHcUm4onQky6I90Ij8VcU=\">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8eK9gPaUDbbTbt0s4m7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSKQw6LrfTmFldW19o7hZ2tre2d0r7x80TZxqxhsslrFuB9RwKRRvoEDJ24nmNAokbwWjm6nfeuLaiFg94DjhfkQHSoSCUbTS/WPP65UrbtWdgSwTLycVyFHvlb+6/ZilEVfIJDWm47kJ+hnVKJjkk1I3NTyhbEQHvGOpohE3fjY7dUJOrNInYaxtKSQz9fdERiNjxlFgOyOKQ7PoTcX/vE6K4ZWfCZWkyBWbLwpTSTAm079JX2jOUI4toUwLeythQ6opQ5tOyYbgLb68TJpnVe+ien53Xqld53EU4QiO4RQ8uIQa3EIdGsBgAM/wCm+OdF6cd+dj3lpw8plD+APn8wcD9I2i</latexit>q1\n<latexit sha1_base64=\"g1udoKqo8BCWu2kpDHD3VWkGU3k=\">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeiF48t2FpoQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgpr6xubW8Xt0s7u3v5B+fCoreNUMWyxWMSqE1CNgktsGW4EdhKFNAoEPgTj25n/8IRK81jem0mCfkSHkoecUWOlJu2XK27VnYOsEi8nFcjR6Je/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSvqh6l9Vas1ap3+RxFOEETuEcPLiCOtxBA1rAAOEZXuHNeXRenHfnY9FacPKZY/gD5/MHxkuM7g==</latexit>a<latexit", "mimetype": "text/plain", "start_char_idx": 2157, "end_char_idx": 3172, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4022fbee-91c7-4511-b72c-a311a8b732f3": {"__data__": {"id_": "4022fbee-91c7-4511-b72c-a311a8b732f3", "embedding": null, "metadata": {"page_label": "3", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b6e267d8-1c49-4c47-a166-2566b6ffc3b5", "node_type": "4", "metadata": {"page_label": "3", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "386860316bf1059941015c2b46a9bbbb3bc5b76fa4e22a3e09fba9e22fae9114", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9f9cdd59-7a12-46d5-9215-e6a61d3c87c5", "node_type": "1", "metadata": {"page_label": "3", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "6d4ae42411c15765fa4518aa30affd431297a22a2d220934005ce72f2aa3820e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c3633805-5a5f-4762-b6a0-3e6c58f1bf94", "node_type": "1", "metadata": {}, "hash": "82dc89a43f95690413224d353cad1e5c6e2c65b9ae4e558676a80a507e6204ea", "class_name": "RelatedNodeInfo"}}, "text": "sha1_base64=\"fVYtN1fO879ZvvbO3E/FpGNntaU=\">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeiF48t2FpoQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgpr6xubW8Xt0s7u3v5B+fCoreNUMWyxWMSqE1CNgktsGW4EdhKFNAoEPgTj25n/8IRK81jem0mCfkSHkoecUWOlZtAvV9yqOwdZJV5OKpCj0S9/9QYxSyOUhgmqdddzE+NnVBnOBE5LvVRjQtmYDrFrqaQRaj+bHzolZ1YZkDBWtqQhc/X3REYjrSdRYDsjakZ62ZuJ/3nd1ITXfsZlkhqUbLEoTAUxMZl9TQZcITNiYgllittbCRtRRZmx2ZRsCN7yy6ukfVH1Lqu1Zq1Sv8njKMIJnMI5eHAFdbiDBrSAAcIzvMKb8+i8OO/Ox6K14OQzx/AHzucPx8+M7w==</latexit>b<latexit sha1_base64=\"6wY00YSWQrna361Pno0aSjaV9sc=\">AAACJXicbVDJSgNBEO1xN26jHr00BkFBwowE9aAgevEYwSyQCUNPpyZp7FnSXSOEIT/jxV/x4sEggid/xc4Crg8KHu9VUVUvSKXQ6Djv1szs3PzC4tJyYWV1bX3D3tyq6SRTHKo8kYlqBEyDFDFUUaCERqqARYGEenB3NfLr96C0SOJb7KfQilgnFqHgDI3k22deGyQyT0KI+z3fOaTMU6LTxYPznu8e0p+2EYIv2/HtolNyxqB/iTslRTJFxbeHXjvhWQQxcsm0brpOiq2cKRRcwqDgZRpSxu9YB5qGxiwC3crHXw7onlHaNEyUqRjpWP0+kbNI634UmM6IYVf/9kbif14zw/C0lYs4zRBiPlkUZpJiQkeR0bZQwFH2DWFcCXMr5V2mGEcTbMGE4P5++S+pHZXc41L5ply8uJzGsUR2yC7ZJy45IRfkmlRIlXDyQJ7ICxlaj9az9Wq9TVpnrOnMNvkB6+MT2yWjnw==</latexit>\u0000(q0,a)=q1,\u0000(q1,", "mimetype": "text/plain", "start_char_idx": 3173, "end_char_idx": 4237, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c3633805-5a5f-4762-b6a0-3e6c58f1bf94": {"__data__": {"id_": "c3633805-5a5f-4762-b6a0-3e6c58f1bf94", "embedding": null, "metadata": {"page_label": "3", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b6e267d8-1c49-4c47-a166-2566b6ffc3b5", "node_type": "4", "metadata": {"page_label": "3", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "386860316bf1059941015c2b46a9bbbb3bc5b76fa4e22a3e09fba9e22fae9114", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4022fbee-91c7-4511-b72c-a311a8b732f3", "node_type": "1", "metadata": {"page_label": "3", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "ca4cd2c791ab1c64381dbc4a833b01bed04d35303d0482f3de4714e646f12374", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "76847fe2-68b2-4d53-8dbb-72eed750f1dd", "node_type": "1", "metadata": {}, "hash": "5c5be86079a541bfa66c3d7042dce0280f7fb6c774a1c7bd59fe6e8c511942fb", "class_name": "RelatedNodeInfo"}}, "text": "a)=q1,\u0000(q1,b)=q0\n<latexit sha1_base64=\"C94fHvK3kIoDIoUjzI9tJYzOzGY=\">AAACKXicbZDLSgMxFIYz9VbrrerSTbAILkqZkaJuCkVBXLZoL9ApQybNtKGZS5MzQhn6Om58FTcKirr1RUwvQm09EPj5/nM4Ob8bCa7AND+N1Mrq2vpGejOztb2zu5fdP6irMJaU1WgoQtl0iWKCB6wGHARrRpIR3xWs4favx37jgUnFw+AehhFr+6QbcI9TAho52bJ9x7s+KdkJyWPXHuXxTckWzAM7GTimLXm3B2NanaN5PHCsX8vJ5syCOSm8LKyZyKFZVZzsq90JaeyzAKggSrUsM4J2QiRwKtgoY8eKRYT2SZe1tAyIz1Q7mVw6wieadLAXSv0CwBM6P5EQX6mh7+pOn0BPLXpj+J/XisG7bCc8iGJgAZ0u8mKBIcTj2HCHS0ZBDLUgVHL9V0x7RBIKOtyMDsFaPHlZ1M8K1nmhWC3mylezONLoCB2jU2ShC1RGt6iCaoiiR/SM3tC78WS8GB/G17Q1ZcxmDtGfMr5/AODMpdU=</latexit>\u2303={a, b},F={q0},Q={q0,q1}\nFigure 2. A demo of DFA recognizing string \u201c (ab)\u2217\u201d.\n3.1.2. C ONVERSATION AS TAG-SEQUENCE\nDFA are widely recognized for their capability to define al-\nphabet sequences with a specific syntax. A classic example\nas shown in Figure 2 is a DFA with a two-state structure\nthat can define strings like \u201c ababab \u201d and \u201c abab \u201d. However,\nwhen it comes to modeling conversations, the challenge\narises due to the complexity and variety of conversational\nelements. Unlike simple syntactic strings, the essence of a\nconversation lies predominantly at the semantic level. For\ninstance, phrases such as \u201c My battery drains out fast \u201d and\n\u201cHow come my phone can be only used for 1 hour? \u201d convey\nthe same underlying issue in a customer service context, yetthey do not share a single word in common. This disparity\nunderscores the limitations of traditional DFA models in\ncapturing the nuances of conversational semantics.\nTo address this, we assume each utterance in a conversa-\ntion can be encapsulated by a set of \u201ckeywords\u201d or \u201ctags\u201d.\n(We use the term \u201c tags \u201d in this paper). For example,\nthe utterance \u201cHow come my phone can be only used\nfor 1 hour?\u201d could be succinctly represented by the tag\nset{\u201c#issues \u201d,\u201c#battery \u201d}in the customer service\ncontext. This method allows for the abstraction of utterances\ninto a more manageable form, suitable for DFA modeling.\n3.1.3. C ONVERSATION SETS AS DFA\nBuilding upon the idea of representing individual conversa-\ntions as sequences of tags, we extend this concept to model\nentire sets of conversations using Deterministic Finite Au-\ntomata (DFAs). Recall our earlier definition of a DFA as a\ntuple (Q,\u03a3, \u03b4, q 0, F). In this context, we adapt these com-\nponents to fit our conversation modeling framework:\n\u2022States (Q): Each state in the DFA represents a particular\nstage or context within a conversation. For instance, a\nstate could represent the initiation of a conversation, a\nquery about a specific issue, a response, or the conclusion\nof the interaction.\n\u2022Alphabet (\u03a3): The alphabet in this model comprises the\nset of all possible tags that we identified as representative\nof different utterances. These tags form the basic building\nblocks of our conversation sequences.", "mimetype": "text/plain", "start_char_idx": 4226, "end_char_idx": 7037, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "76847fe2-68b2-4d53-8dbb-72eed750f1dd": {"__data__": {"id_": "76847fe2-68b2-4d53-8dbb-72eed750f1dd", "embedding": null, "metadata": {"page_label": "3", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b6e267d8-1c49-4c47-a166-2566b6ffc3b5", "node_type": "4", "metadata": {"page_label": "3", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "386860316bf1059941015c2b46a9bbbb3bc5b76fa4e22a3e09fba9e22fae9114", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c3633805-5a5f-4762-b6a0-3e6c58f1bf94", "node_type": "1", "metadata": {"page_label": "3", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "1b7529e03949e19784f669aeadb97c6e11feafcc6e6c452eb6426015e6305f88", "class_name": "RelatedNodeInfo"}}, "text": "These tags form the basic building\nblocks of our conversation sequences.\n\u2022Transition Function (\u03b4): The transition function maps\na state and a tag to a subsequent state. It encapsulates\nthe flow of conversation, determining how an utterance\n(through its tags) leads from one conversational context\nto another.\n\u2022Start State (q0): The start state represents the beginning\nof a conversation. It could be a generic greeting or an\ninitial query, setting the stage for the interaction.\n\u2022Accept States (F): These are the states indicating the\ncompletion of a conversation. An accept state could be\nreached after successfully addressing a query, reaching\na satisfactory conclusion, or when the conversation natu-\nrally comes to an end.\nRemark on the dialogue tracking function I.In our DFA\nframework, a unique functionality is incorporated where\neach state maintains a record of the indices of dialogues that\nhave traversed through it. This is accomplished by tracing\nthe trajectory of tag sequences as the conversation unfolds.\nFormally, this tracking is represented by the function I(q),\nwhich maps a state qto a set of dialogue IDs in {1, ..., N}.\nCrucially, it enables us to trace back and identify the most\nrelevant dialogue samples that can be effectively utilized in\nin-context learning.\n3", "mimetype": "text/plain", "start_char_idx": 6965, "end_char_idx": 8252, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "64d15b93-d8a9-4b51-8302-e377fa135abe": {"__data__": {"id_": "64d15b93-d8a9-4b51-8302-e377fa135abe", "embedding": null, "metadata": {"page_label": "4", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "77bb7dc3-71c2-4862-9653-1c6a188f9589", "node_type": "4", "metadata": {"page_label": "4", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "006075c3536fd3a930cef786c5df66c31a3e897c69b31edd1c3eaac5c72b5308", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "82ea3b5a-aaf4-4b6f-971b-3b683badc115", "node_type": "1", "metadata": {}, "hash": "83ba2967f64deba675efe983326df3e7c3c5bc5e962b5a882fee30fe069e00f5", "class_name": "RelatedNodeInfo"}}, "text": "DFA-RAG: Conversational Semantic Router for Large Language Model with Definite Finite Automaton\n15Breakpoint#keyboard #issue#link#thank#battery  #issue#link#thank#update#link#thank\n012RoundID012012253748Tags\n\u03a8[0] = {25: {\"#keyboard\", \"#issue\"},        37: {\"#battery\", \"#issue\"},        48: {\"#update\"}}\u03a8[1] = {25: {\"#link\"},        37: {\"#link\"},        48: {\"#link\"}}\u03a8[2] = {25: {\"#thank\"},        37: {\"#thank\"},        48: {\"#thank\"}}t = \"#issue\"0  # INPUT: Tag Sequences   # OUTPUT: DFA-tuple (Q, \u03a3, \u03b4, q0, F)1  \u03a8 \u2190 collect tag sets in all rounds    # Construct tree in each round2  for round in range(0, MAX_ROUND):3    if round == 0: 4      Q = {q0} 5    else:6      Q \u2190 leaf-states in previous round7    for q in Q8      build(q, \u03a8[round][q.IDs])9  def build(q, S)10   if S is empty then return11   t \u2190 Get the most frequent tag t in S12   q\u2019 \u2190 \u03b4(q, t) # create children state13   S\u2019 \u2190 tag sets in S with tag t14   Remove t from tag sets in S\u201915   build(q\u2019, S\u2019)16   build(q, S \\ S\u2019)Algorithm 1: Build a Tree-like Initial DFA with Tag Sequences \nBreakpointINPUT: Tag Sequences\n11\n1Breakpoint\n12q0issue\nq0issuekeyboardbattery\nBreakpoint\n16q0issuekeyboardbatteryupdateS \\ S\u2019 = {48:{\"#update\"}}\nFinal Stageq0issuekeyboardbatteryupdate\nlinklinklinkthankthankthank... ...round = 2round = 0\n13S\u2019 = {25: {\"#keyboard\", \"#issue\"},    37: {\"#battery\", \"#issue\"},}BreakpointBreakpointS\u2019 = {{25:\"#keyboard\"},37:{\"#battery\"}}\n14Breakpoint\nFigure 3. Illustration of the algorithm for building a tag tree, expressed in Python-style pseudocode. (1)Positioned centrally is the core\nalgorithm, flanked on either side by demonstrative examples that \u201cprint out\u201d the algorithm\u2019s internal variables at various \u201cbreakpoints\u201d.\n(2)On the left, we begin by displaying the input data used in the demonstration. Following this, the first breakpoint, corresponding to Line\n1of the algorithm, showcases the variables storing the ID-ed tag sets across different conversational rounds. (3)Shifting to the right side,\nwe illustrate the evolution of the variables and the tree structures, starting from round 0 with the initial state q0. The breakpoints at Lines\n15&16forego stepping into the function, instead presenting the completed tree structures directly for clarity. To simplify the illustration,\nwe omit the iterative process for rounds 1 &2, jumping straight to the final results.\n3.2. Learn DFA from Conversations\nThis section delves into the methodology of learning DFA\nfrom conversation sets, with a brief outline in Figure 4.\nIn Section 3.2.1, we detail the process of deriving tag se-\nquences from conversational data, employing LLMs to ac-\ncurately identify and extract relevant tags. Following this,\nSection 3.2.2 focuses on the assembly of these tag sequences\ninto a structured tree format, laying the groundwork for DFA\nconstruction. Lastly, in Section 3.2.3, we introduce the state-\nmerging process within the tag tree, refining the DFA to\nsuccinctly represent the dynamics of conversational flows.\n#keyboard #issue#link#thank\n#keyboard #issue#link#thank\n#keyboard #issue#link#thank\nRaw DialoguesTag SequencesInitial DFA (Tag-Tree)Final DFA(Merged) Sec. 3.2.1Sec. 3.2.2Sec. 3.2.3\nFigure 4. Outline for Section 3.2.\n3.2.1. E XTRACTING TAGSEQUENCES FROM\nCONVERSATIONS USING LLM S\nIn this subsection, we detail the method for converting a\nconversation into a sequence of tags, a crucial preprocessing\nstep for constructing the DFA.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3420, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "82ea3b5a-aaf4-4b6f-971b-3b683badc115": {"__data__": {"id_": "82ea3b5a-aaf4-4b6f-971b-3b683badc115", "embedding": null, "metadata": {"page_label": "4", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "77bb7dc3-71c2-4862-9653-1c6a188f9589", "node_type": "4", "metadata": {"page_label": "4", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "006075c3536fd3a930cef786c5df66c31a3e897c69b31edd1c3eaac5c72b5308", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "64d15b93-d8a9-4b51-8302-e377fa135abe", "node_type": "1", "metadata": {"page_label": "4", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "0babe36891331373bb928f7649903f3319d9253f0331947b6e08ad11da9f744d", "class_name": "RelatedNodeInfo"}}, "text": "The process hinges on theuse of the state-of-the-art LLMs, such as GPT-4, which\nare adept at processing natural language and extracting key\ninformation like human beings. The primary task of the\nLLM in this context is to preprocess each sentence of a\nconversation and distill it into the most representative tags.\nThe extraction process is guided by specific prompt param-\neters to ensure that the resulting tags are both concise and\nrelevant to the conversational content. These prompts are\nstructured to instruct LLM adhering to two main criteria:\na)Brevity : The LLM is instructed to use as few words as\npossible, ideally limiting the output to no more than three\nwords per tag. b) Focus on Core Elements : The summa-\nrization is directed to concentrate on actual events, issues,\nqueries, or solutions present in the conversation. We include\nthe full prompts in Appendix A.\nBy applying the prompts, the LLM processes each sen-\ntence of the conversation and outputs succinct, focused tags.\nThese tags serve as the building blocks for constructing the\nDFA, encapsulating the key elements of the conversation.\n3.2.2. T REE CONSTRUCTION WITH TAGSEQUENCES\nThe traditional approach to building a DFA often begins\nwith the construction of a prefix tree, representing the se-\nquences of alphabets that form valid strings in the language.\nIn the case of conversation sets, however, we encounter\n4", "mimetype": "text/plain", "start_char_idx": 3421, "end_char_idx": 4811, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "21812bc7-cdc8-428f-96be-c54e3263dc38": {"__data__": {"id_": "21812bc7-cdc8-428f-96be-c54e3263dc38", "embedding": null, "metadata": {"page_label": "5", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5f687132-f180-41f6-81f4-73289528c737", "node_type": "4", "metadata": {"page_label": "5", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "dc234113ad293ce04504ae3f1c6393085b7267043cd136c86ae60c82fb258dc7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "db73b87d-b7ed-44a4-afca-4738c55a929d", "node_type": "1", "metadata": {}, "hash": "d96af9664773e55d582f36e97c3a4ab249781c643c461a82ec5c7382be9b12e5", "class_name": "RelatedNodeInfo"}}, "text": "DFA-RAG: Conversational Semantic Router for Large Language Model with Definite Finite Automaton\ntwo significant challenges that diverge from this traditional\nmethod:\n1. Tags in each utterance can appear in various orders ,\nwhich is unlike strings in traditional DFA models that fol-\nlow a fixed sequence of alphabets. This flexible ordering\npresents a unique challenge in constructing a prefix tag tree.\nFor example, in a standard DFA, the string \u201cab\u201d is distinctly\nrepresented by the sequence \u201c a\u2192b\u201d. However, a sentence\nlike \u201cMy iPhone battery drains fast\u201d in a conversation can\nbe equivalently tagged as either \u201c# issues \u2192#battery \u201d\nor \u201c#battery \u2192#issues \u201d. To efficiently manage this\nvariability and reduce the size of the tag tree, our algorithm\nprioritizes tags based on their frequency of occurrence, pre-\nferring to place more frequent tags earlier in the sequence.\n2. Each tag derived from an utterance is associated with\nthe context of a specific round in the conversation . For\ninstance, the tag \u201c# address \u201d may carry distinct semantic\nmeanings in the user round compared to the system round.\nIn one context, it might signify an address query, while in\nanother, it serves to provide information. Therefore, when\nevaluating tag frequencies or constructing the tree, we need\nto focus exclusively on tags from the same conversational\nround. As illustrated in the left part of Figure 3, an important\npreprocessing step is to convert the original tag sequences to\na special data structure \u03a8with ID-ed tag sets across different\nconversational rounds.\nTo address these challenges, we introduce the algorithm as\ndepicted in Figure 3. It begins with constructing a tag tree\nfrom the initial state q0andround 0 , detailed from Lines 2 to\n8. For each subsequent round, the newly constructed tree\u2019s\nroot state becomes a child state of one of the leaf states from\nthe previous round\u2019s tree. The selection of leaf states Q\nfor progression into the next round is guided by additional\nheuristic rules, which are elaborated in Appendix B. The\nprocedure for building a tree from a set of tags, along with\nassociated dialogue IDs, is expanded upon in Lines 9 to\n16. The central strategy is recursively selecting the most\nfrequent tag to expand the tree.\nUpon execution of Algorithm 1, we yield a tree-like struc-\nture resembling a DFA. However, during our final results\ndemonstration in the right-bottom part of Figure 3, it\nbecomes evident that duplicated sub-sequences, such as\n\u201c#link \u2192#thank \u201d, exist. This redundancy presents an\nopportunity to merge these sequences, thereby reducing the\noverall size of the resultant DFA.\n3.2.3. S TATE MERGING IN TAGTREE\nIn the process of constructing a DFA-tree from conversa-\ntion sets, the variability in the tags generated by LLM for\nutterances with similar or identical contexts is a key chal-\nlenge. For instance, tags like \u201c #subscription \u201d and\u201c#membership \u201d might represent the same conversation\ntopic, yet linguistically, they are distinct. Merging such\nstates based purely on linguistic similarities is impractical\ndue to their semantic differences. However, in a conver-\nsational context, these tags often play similar roles and\nare connected to related sub-structures, such as common\nchildren-tags like \u201c #refund \u201d or \u201c#payment \u201d, which are\ntypically associated with financial aspects of subscriptions.\nTo address this, we introduce a similarity score \u03d5sim(q, q\u2032)\nbetween two states qandq\u2032as follows:\n\u03d5sim(q, q\u2032) =P\nt\u2208T(q)\u2229T(q\u2032)|I(\u03b4(q, t))| \u00b7 |I(\u03b4(q\u2032, t))|\nP\nt\u2208T(q)|I(\u03b4(q, t))| \u00b7P\nt\u2208T(q\u2032)|I(\u03b4(q\u2032, t))|,\nwhere we define function Tas the tag sets connected to q:\nT(q) ={t\u2208\u03a3| \u2203q\u2032\u2208Q, \u03b4(q, t) =q\u2032}\nIntuitively, \u03d5sim(q, q\u2032)calculates the ratio of common tags\nconnected to both states qandq\u2032. The significance of each\ntag is considered by counting the number of dialogues re-\nlated to the connected state, denoted by |I(\u03b4(q, t))|.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3867, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "db73b87d-b7ed-44a4-afca-4738c55a929d": {"__data__": {"id_": "db73b87d-b7ed-44a4-afca-4738c55a929d", "embedding": null, "metadata": {"page_label": "5", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5f687132-f180-41f6-81f4-73289528c737", "node_type": "4", "metadata": {"page_label": "5", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "dc234113ad293ce04504ae3f1c6393085b7267043cd136c86ae60c82fb258dc7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "21812bc7-cdc8-428f-96be-c54e3263dc38", "node_type": "1", "metadata": {"page_label": "5", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "46679db05e95e7fbee8fc26a5e73ea9f364782b4b74810945a726a956988a848", "class_name": "RelatedNodeInfo"}}, "text": "This\napproach focuses on comparing the immediate (1-hop) chil-\ndren\u2019s states. Empirical evidence suggests that comparing\ndirect children often suffices, though the formula can be\nextended to compare deeper sub-tree structures if necessary.\nOnce the similarity score is calculated, we set a threshold \u03bb\nto determine which node pairs should be merged. Pairs with\na similarity score exceeding this threshold, \u03d5sim(q, q\u2032)>\n\u03bb, are considered for merging. This methodology allows\nus to consolidate similar conversational contexts, reducing\nredundancy and increasing the efficiency of the DFA-tree.\n3.3. Conversation Generation by DFA-RAG\nOnce a DFA is constructed to model conversation sets, it can\nbe integrated with a Large Language Model (LLM) to gen-\nerate dynamic and coherent conversations as demonstrated\nin Figure 1. This process involves a cyclical interaction\nbetween the user\u2019s inputs, the DFA\u2019s navigational structure,\nand the LLM\u2019s response generation capabilities. Here are\nthe concrete steps involved in this procedure:\n1.Tagging User Utterances : When a user responds, their\ninput is first processed by the LLM, which converts it\ninto tags. This process mirrors the approach used for\ntagging the training samples, ensuring consistency in\ntagging the conversational content.\n2.Navigating the DFA : Based on the tags derived from\nthe user\u2019s input, the system navigates through the DFA\nwith a deterministic path to identify the corresponding\nstate. Given an input tag sequence {u0, u1, ..., u T}, the\nstate transition at step tis denoted as qt=\u03b4(qt\u22121, ut).\n5", "mimetype": "text/plain", "start_char_idx": 3868, "end_char_idx": 5433, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "760f5448-22ac-477b-ad23-f88ed331c0f3": {"__data__": {"id_": "760f5448-22ac-477b-ad23-f88ed331c0f3", "embedding": null, "metadata": {"page_label": "6", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4e5849c6-12e9-4373-9da0-5decbb86017e", "node_type": "4", "metadata": {"page_label": "6", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "32bfda686f315f23fc81c312dfdb47bc30a58ea627f43e2d53ebad6e76d4f073", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0b0ebff4-a6fb-4d3f-a597-2fa254cc5360", "node_type": "1", "metadata": {}, "hash": "7bdeebee41a1cbb3fdd06f7c1e29a391ddf452085c2597c0bf198f52f2d49d23", "class_name": "RelatedNodeInfo"}}, "text": "DFA-RAG: Conversational Semantic Router for Large Language Model with Definite Finite Automaton\nTable 1. Results of dialogue generation quality across different base models and methods. This table reports the \u201cWin Rate\u201d over naive\nbase models (GPT-4, GPT-3.5) regarding dialogue generation performance. For each method using in-context learning ( RandSamp ,RAG,\nBM25 ,DFA-RAG ), we use 5 samples in the inference time. For FT-LLM , we perform fine-tuning using the API provided by OpenAI\nwith standard hyperparameters. Note that the API for fine-tuning GPT-4 is not available.\nBase LLM MethodsDomainsAverageAmazonHelp DeltaSupport AskPlayStation AirbnbHelp NikeSupport CambridgeInfo\nGPT-4RandSamp 69.1 84.1 57.9 78.3 45.3 67.0 66.9\nBM25 67.3 81.5 63.8 77.1 59.8 63.0 68.7\nRAG 74.4 87.0 66.3 72.2 57.3 66.5 70.6\nFT-LLM - - - - - - -\nDFA-RAG (Ours) 78.0 89.9 65.9 80.9 62.6 68.5 74.3\nGPT-3.5RandSamp 70.2 83.6 61.3 69.5 58.9 57.9 66.9\nBM25 70.6 84.1 64.7 74.3 60.4 58.8 68.8\nRAG 73.8 82.9 72.4 76.6 63.3 60.6 71.6\nFT-LLM 69.7 64.6 71.7 66.1 56.8 56.1 64.2\nDFA-RAG (Ours) 78.5 89.8 72.9 79.1 70.1 64.9 75.9\nIn cases where \u03b4(qt\u22121, ut) =\u2205, indicating a mis-\nmatch, the navigation process concludes, and we pro-\nceed to retrieve dialogue IDs based on the last valid\nstate, qt\u22121.\n3.Accessing Dialogue IDs : Each state qin the DFA\nis associated with a collection of dialogue IDs I(q),\nwhich represent past conversations that have followed\na similar pathway through the DFA.\n4.Compiling a Prompt for LLM : From the dialogue\nIDs, the system samples 5examples and compiles\nthem into a prompt structured for in-context learning\nas shown in Figure 1(c).\n5.LLM Response Generation : The LLM then generates\na response based on the given prompt.\n6.Iterative Process : The conversation continues with the\nuser providing their next response based on the LLM\u2019s\noutput. The process reverts to STEP 1 , with the user\u2019s\nnew response undergoing the same cycle of tagging,\nDFA navigation, and LLM response generation.\nThis iterative process allows for a dynamic conversation\nflow, where each response is contextually informed and\nsemantically relevant.\n4. Experimental Results\nDatasets. We carry out experiments with dialogue datasets\nin extensive domains. A high-level summary of the dataset\nstatistics is provided in Table 2.\nThe dataset described in the table consists of domain-\nspecific collections, each serving distinct business com-\nmunication needs. Notably, the first five datasets \u2014\nAmazonHelp ,DeltaSupport ,AskPlayStation ,\nAirbnbHelp , and NikeSupport , are derived from\nTwitter interactions (Axelbrooke, 2017), including a range\nof sizes from larger datasets containing over 50,000 dia-Table 2. Summary statistics of the datasets.\nDomains Dataset Size Test Size Avg. Rounds Source\nAmazonHelp 50K 1K 6.4 Tweet\nDeltaSupport 20K 1K 4.4 Tweet\nAskPlayStation 10K 500 3.8 Tweet\nAirbnbHelp 3K 100 4.2 Tweet\nNikeSupport 1K 100 5.5 Tweet\nCambridgeInfo 8K 1k 13.5 MultiWOZ\nlogues (AmazonHelp and DeltaSupport) to smaller ones\nwith 1,000 entries (NikeSupport).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3040, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0b0ebff4-a6fb-4d3f-a597-2fa254cc5360": {"__data__": {"id_": "0b0ebff4-a6fb-4d3f-a597-2fa254cc5360", "embedding": null, "metadata": {"page_label": "6", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4e5849c6-12e9-4373-9da0-5decbb86017e", "node_type": "4", "metadata": {"page_label": "6", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "32bfda686f315f23fc81c312dfdb47bc30a58ea627f43e2d53ebad6e76d4f073", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "760f5448-22ac-477b-ad23-f88ed331c0f3", "node_type": "1", "metadata": {"page_label": "6", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "8b0f47018fb8fbf5a87f55b435d0b6c0dd2601c6d5fe8e3a347c433403010f0a", "class_name": "RelatedNodeInfo"}}, "text": "These datasets represent\nunique customer service interactions pertaining to their re-\nspective single business domains.\nIn contrast, CambridgeInfo , an alias for the Cambridge\nInformation Center, encompasses a broader spectrum of ser-\nvices. It includes dialogues related to various booking ser-\nvices such as train, taxi, and hotel reservations. This dataset\nis sourced from MultiWOZ (Budzianowski et al., 2018),\nand differs significantly from the Twitter-based datasets.\nMultiWOZ is known for its dense annotations, detailing the\npurpose and key information of each dialogue. This feature\nof MultiWOZ allows for a more nuanced analysis of dia-\nlogues in Section 4.2, particularly in assessing the success\nrate of interactions.\n4.1. Generation Quality Evaluation\nThis section aims to evaluate the quality of generated di-\nalogues using the DFA-RAG framework. We compare\nperformance on major pretrained LLMs as the backbone\nmodel in generating dialogues: GPT-4 (OpenAI, 2023),\nGPT-3.5 (Ouyang et al., 2022). The concrete versions are\ngpt-4-1106-preview andgpt-3.5-turbo-1106 respectively.\nEvaluation Technique . We leverage GPT-4 for the pri-\nmary evaluation, given its superior performance metrics.\nAccording to Zheng et al. (2023), GPT-4 demonstrates\nan 85% agreement rate with human evaluators, signifying\n6", "mimetype": "text/plain", "start_char_idx": 3041, "end_char_idx": 4351, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0a581623-32a7-4b0a-af17-10401dae3dc7": {"__data__": {"id_": "0a581623-32a7-4b0a-af17-10401dae3dc7", "embedding": null, "metadata": {"page_label": "7", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e71b2695-f5dc-4ee1-a004-eb1e2fd0a09c", "node_type": "4", "metadata": {"page_label": "7", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "6db8beee9bd54e5ded014368f6672fd89f632eab4d5545845f120b61ae0446c1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a31faf5a-76a0-48dc-86b1-1dada8caf085", "node_type": "1", "metadata": {}, "hash": "e175e63b22c751bb9dff87ca99efe7fbab458ccb7f35a98f9b19c6b8388ba9df", "class_name": "RelatedNodeInfo"}}, "text": "DFA-RAG: Conversational Semantic Router for Large Language Model with Definite Finite Automaton\nits reliability in assessing dialogue quality. We follow the\nmethodology outlined in AlpacaEval (Dubois et al., 2023)\nwith similar prompts and metrics as detailed in Appendix A.\nThe evaluation process involves GPT-4 comparing two dia-\nlogues to determine which one is closer to the ground truth.\nThe dialogues completed directly by their respective LLMs\nserve as the target to be compared. The effectiveness of each\nmethod is quantified using the \u201c Win Rate \u201d score, which\nreflects how often a dialogue surpasses the quality of the\ncompetitor. For a qualitative comparison, we also present ex-\nample dialogues with respective comments from the GPT-4\nevaluator in Appendix C.\nBaselines . We compare diverse strategies in selecting in-\ncontext learning (ICL) examples, recognizing the sensitiv-\nity of few-shot learning to sample selection: a) Random\nSampling involves randomly selecting examples from the\ntraining dataset; b) RAG utilizes text-embedding-3-small\n(from OpenAI) for generating sentence embeddings and fo-\ncuses on retrieving training examples with utterances most\nsimilar to the test utterance; c) BM25 is a classical sparse re-\ntrieval method used to find training examples with utterances\nclosely matching those in the test scenarios. Additionally,\nwe examine another baseline, \u201c FT-LLM \u201d (Finetuned Large\nLanguage Model), which involves directly finetuning the\nLLM on the dialogues in the training set.\nObservations . Our observations from the evaluation in\nTable 1 reveal some notable trends and insights: (a) the\napproach of fine-tuning ( FT-LLM ) showed underwhelming\nperformance, which can be attributed to the limited volume\nof domain-specific training data, such as NikeSupport; (b)\nRAG demonstrates stronger performance than RandSamp\nandBM25 , which is likely due to its more accurate retrieval\nof relevant samples; (c) DFA-RAG outperforms the best\nbaseline by 4%at win-rate. Note that DFA-RAG not only\nfacilitates the generation of high-quality dialogues but also\nprovides a clear and interpretable structure, making it easier\nfor humans to understand and analyze the dialogue flow.\n4.2. Dialogue Task Evaluation\nIn the evaluation of dialogue systems, particularly within\ncustomer service domains, one key metric is the system\u2019s\nability to resolve the user\u2019s inquiries and issues effectively.\nOur evaluation is performed on the MultiWOZ dataset (cor-\nresponds to CambridgeInfo domain in Section 4.1), a\nwidely recognized benchmark in the realm of task-oriented\ndialogue generation.\nIn these dialogues, system responses are typically presented\nin a delexicalized form \u2014 specific values for certain vari-\nables like time/address within the dialogue are replaced with\nplaceholders. In line with this practice, our model also in-\ncorporates delexicalization by replacing specific examples\nin in-context learning with placeholder forms.To quantitatively measure the performance of our model, we\nemploy standardized evaluation metrics, namely \u201c Inform \u201d\nand \u201c Success \u201d rates. The \u201cInform\u201d rate measures the de-\ngree to which the system provides sufficient and relevant\ninformation to fulfill the user\u2019s information needs, while the\n\u201cSuccess\u201d rate evaluates the system\u2019s performance in terms\nof completing the user\u2019s goal like booking a hotel. The goal\nis to understand the overall efficacy of the dialogue system\nin achieving the intended outcomes of the interaction.\nTable 3. Evaluation results on the task-oriented dialogues.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3542, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a31faf5a-76a0-48dc-86b1-1dada8caf085": {"__data__": {"id_": "a31faf5a-76a0-48dc-86b1-1dada8caf085", "embedding": null, "metadata": {"page_label": "7", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e71b2695-f5dc-4ee1-a004-eb1e2fd0a09c", "node_type": "4", "metadata": {"page_label": "7", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "6db8beee9bd54e5ded014368f6672fd89f632eab4d5545845f120b61ae0446c1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a581623-32a7-4b0a-af17-10401dae3dc7", "node_type": "1", "metadata": {"page_label": "7", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "81331ccb769051548c163e45ec9c4ffdb3db44447bbfa4b8faaca8029b7118ff", "class_name": "RelatedNodeInfo"}}, "text": "Table 3. Evaluation results on the task-oriented dialogues.\nGround Truth States\nin Training?Methods Inform Success\nYesHDSA 87.9 79.4\nMarCo 94.5 87.2\nHDNO 93.3 83.4\nGALAXY 92.8 83.5\nKRLS 93.1 83.7\nNoAuGPT 76.6 60.5\nMTTOD 85.9 76.5\nRSTOD 83.5 75.0\nRewardNet 87.6 81.5\nTOATOD 90.0 79.8\nNo DFA-RAG (Ours) 93.3 90.0\nIn Table 3, we compared two sets of baseline meth-\nods: a) one branch of works (HDSA (Chen et al., 2019),\nMarCo (Wang et al., 2020b), HDNO (Wang et al., 2020a),\nGALAXY (He et al., 2022), KRLS (Yu et al., 2023)) as-\nsumes known dialogue states for policy optimization strat-\negy and response generation; b) the other branch, also\nknown as End-to-End models (AuGPT (Kulh \u00b4anek et al.,\n2021), MTTOD (Lee, 2021), RSTOD (Cholakov & Kolev,\n2022b), RewardNet (Feng et al., 2022), TOATOD (Bang\net al., 2023)), rely solely on dialogue context. Our method\nfalls into the latter category, focusing on the model\u2019s in-\nherent ability to generate responses without using known\ndialogue states. Despite this, our approach has delivered\npromising results across both \u201cInform\u201d and \u201cSuccess\u201d met-\nrics, exhibiting performance comparable to methods relying\non ground truth dialogue states. This highlights the effi-\ncacy and adaptability of our model in handling a range of\ndialogue scenarios efficiently.\n4.3. Constructed DFA Demonstrations\nWe display the (partial) DFA constructed from the Mul-\ntiWOZ dataset in Figure 5. At the starting point of the\nautomaton, we observe direct connections to several key\ndomains such as \u201c train \u201d, \u201chotel \u201d, and \u201c taxi \u201d. These\ndomains represent the most frequently encountered topics in\nthe MultiWOZ dataset. As we delve into specific domains\nlike restaurants and hotels, a notable observation is the re-\ncurrence of similar tags such as \u201c price \u201d, \u201cname \u201d, and\n\u201carea \u201d, which are common queries from users. Another\n7", "mimetype": "text/plain", "start_char_idx": 3483, "end_char_idx": 5331, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "71017e2f-415d-4348-b708-8b0e34a8dfe5": {"__data__": {"id_": "71017e2f-415d-4348-b708-8b0e34a8dfe5", "embedding": null, "metadata": {"page_label": "8", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0299c721-d0c8-4bae-a221-b8008e112839", "node_type": "4", "metadata": {"page_label": "8", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "786e9e3b89c5059da5f193a133b0ad4a1f3aa355901ea6d9fab581d39d903e44", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "91e2a0ab-3657-4260-8cbc-34b0e827b9fa", "node_type": "1", "metadata": {}, "hash": "e7699da74f0ffe6b3f44b8d714b3386f87f602bbe81c6f1617731ecb91fef9c4", "class_name": "RelatedNodeInfo"}}, "text": "DFA-RAG: Conversational Semantic Router for Large Language Model with Definite Finite Automaton\nIDRestaurantFoodHospitalHotelAttractionTrainNamePriceNoHelp\nWifiAreaParkingTypeAccidentTaxiStaySearchFindAlternative\nCountNameLocationAddressThanksGoodbye\u2026\u2026\u2026\u2026\u2026\u2026\n\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u20260 USER: I want to \ufb01nd an expensive restaurant and serves european food. Can i also have the address, phone number and its area?1 SYSTEM: Eraina is a European restaurant in the centre area. Their address is St. Michael's Church Trinity Street City Centre and phone number 01223 355166.2 USER: thank you goodbye3 SYSTEM: Thank you and good bye0 USER: I want a moderately priced restaurant and serves asian oriental food. What is the address and phone number?1 SYSTEM: there is an Asian Oriental place at 40428 King Street City Centre. You can call them at 01223 518111.2 USER: Thank you goodbye.3 SYSTEM: Thank you for using our system! Have a great day!0 USER: I want a cheap portuguese restaurant. Give me their address, phone number, and area.1 SYSTEM: Nandos is a cheap Portuguese restaurant in the south area. The phone number is 01223 327908 and the address is Cambridge Leisure Park Clifton Way2 USER: Thank you, goodbye3 SYSTEM: You're welcome, goodbye.1182387350Raw Dialogues in Training Set\n(a) An Example Tag Pathway in Constructed DFA(b) Corresponding Dialogues in the Training Set \nFigure 5. DFA Results for MultiWOZ. (a) This segment of the figure illustrates a portion of the constructed DFA. The black circle\nindicates the starting point of the automaton. Each green circle represents a \u201cuser\u201d state, while each blue circle denotes a \u201csystem\u201d state.\nThe states are interconnected by arrows, each labeled with a tag. Note that some lines are interconnected (ex. lines correspond to\u201cname\u201d\nand \u201carea\u201d), it means that the relevant nodes are connected in both ways. (b) A specific path within the DFA is highlighted to demonstrate\nits correspondence with actual dialogues traversed. In these dialogues, elements associated with the tags are emphasized in bold.\nintriguing aspect of the DFA is how the same state can lead\nto divergent system responses based on the user\u2019s input and\nthe system\u2019s capabilities. For instance, if the system success-\nfully resolves a user\u2019s inquiry, as illustrated in the figure, it\nproceeds to provide specific details like the concrete address\ninformation. Conversely, another branch emerges when the\nsystem fails to find the desired target, signified by a \u201cno\u201d\ntag. For readers\u2019 interest, we include more DFA demos\nin Appendix D. Overall, the DFA illustration captures the\nmultifaceted nature of conversational flows within the Mul-\ntiWOZ dataset and serves as a potent tool for understanding\nand improving domain-specific dialogue systems.\n5. Discussions and Future Work\nHandling Out-of-distribution Utterances . In practice,\nit is common to encounter out-of-distribution (OOD) in-\nstances (Yang et al., 2022; Zhang et al., 2023; Sun, 2023;\nSun et al., 2023; 2024) that deviate from the pre-defined\ntrajectory in the DFA constructed from training dialogues.\nUnlike existing research in OOD detection (Du et al., 2024;\nGhosal et al., 2024; Ming et al., 2022b;a), which typically\nrejects user input when an OOD instance is detected, our\napproach leverages the dynamic capabilities of LLMs to\nrespond to OOD utterances based on their best judgment\u2014a\nfeature absent in traditional rule-based dialogue systems.\nFormally, utterances are treated as OOD in dialogues where\n\u03b4(qt\u22121, ut) =\u2205, indicating no viable subsequent state (a\nmismatch scenario), the framework concludes the current\nnavigation process and retrieves dialogue options based on\nthe most recent valid state, qt\u22121. This approach addresses\nsituations where user inputs do not match any predefinedpathway in the DFA, while still managing to navigate to\nthe most relevant context. For example, if an unfamiliar\nuser\u2019s inquiry occurs\u2013such as \u201cbooking NBA game tickets\u201d,\nwhich might not directly correspond to a NBA tag within the\nDFA\u2019s structure, our framework adeptly guides the dialogue\ntowards a related, more general context.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4090, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "91e2a0ab-3657-4260-8cbc-34b0e827b9fa": {"__data__": {"id_": "91e2a0ab-3657-4260-8cbc-34b0e827b9fa", "embedding": null, "metadata": {"page_label": "8", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0299c721-d0c8-4bae-a221-b8008e112839", "node_type": "4", "metadata": {"page_label": "8", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "786e9e3b89c5059da5f193a133b0ad4a1f3aa355901ea6d9fab581d39d903e44", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "71017e2f-415d-4348-b708-8b0e34a8dfe5", "node_type": "1", "metadata": {"page_label": "8", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "1a3e193428f95018ae32c6fdfa84b42e3c42224899c1e029a9a156e5171aa609", "class_name": "RelatedNodeInfo"}}, "text": "This is achieved\nby reverting to the parent state qt\u22121which represents sports\nevent ticket booking. This strategy ensures the DFA-RAG\ncan maintain coherence and relevance in conversations, even\nwhen faced with partially matched or unexpected user in-\nputs.\nIntegration with External Modules. The plug-and-play\nnature of our method facilitates seamless integration with\nexisting services and algorithms. For instance, in scenarios\nwhere an input sentence\u2019s tags fall outside the predefined\ntracks of the DFA, our framework can dynamically interface\nwith web search services or other external tools via callable\nAPIs. This enables the LLM to access a broader range of in-\nformation and resources to improve the quality of generated\nresponses.\n6. Related Work\nStructured Dialogue System. Traditional conversational\nagents operate on a fixed knowledge base, providing de-\nterministic responses to user inputs from a pre-defined set\nof answers (Abdul-Kader & Woods, 2015; Bickmore et al.,\n2016; Jain et al., 2018; Medeiros et al., 2019; Hussain et al.,\n2019; Adamopoulou & Moussiades, 2020; Janssen et al.,\n2020; Safi et al., 2020). While reliable and consistent, they\nlack flexibility and struggle with unexpected queries. In\ncontrast, ML-based conversational agents (OpenAI, 2023;\n8", "mimetype": "text/plain", "start_char_idx": 4091, "end_char_idx": 5370, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dc71cc6f-3115-4644-bfe5-acf79adb2a26": {"__data__": {"id_": "dc71cc6f-3115-4644-bfe5-acf79adb2a26", "embedding": null, "metadata": {"page_label": "9", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ee28887d-94b6-4892-b0f8-0d3ce82210ab", "node_type": "4", "metadata": {"page_label": "9", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "20c59a918bb7d074524b95b765f367d21589e021549777692717eb73d95130d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b3ba7b87-ccaa-4e9e-93e4-7a56ce10ebcf", "node_type": "1", "metadata": {}, "hash": "9b6abb61b8a4d3361eb2782b556600ef199b2512045d2db038dc44de1985a5ef", "class_name": "RelatedNodeInfo"}}, "text": "DFA-RAG: Conversational Semantic Router for Large Language Model with Definite Finite Automaton\nOuyang et al., 2022), represent a more adaptive approach\nbut can be unpredictable. Our proposed method combines\nthe stability of traditional agents with the adaptability of\nML-based systems, leveraging the strengths of both to create\na more dynamic and reliable dialogue system.\nRetrieval-Augmented Generation (RAG). RAG was origi-\nnally introduced in (Lewis et al., 2020), which established\nan embedding database containing accessible documents,\nenabling the retrieval of related information in generat-\ning responses. This approach addresses key limitations\nof LLMs, particularly in areas of knowledge updating and\nreasoning transparency (Shuster et al., 2021; Yasunaga et al.,\n2022; Borgeaud et al., 2022; Khattab et al., 2022; Cheng\net al., 2022; Wang et al., 2023c; Cheng et al., 2023). Build-\ning upon the naive RAG, more sophisticated variants have\nbeen developed. These models incorporate complex compo-\nnents like structured data sources. For instance, RET-LLM\n(Modarressi et al., 2023) constructs a personalized knowl-\nedge graph memory, extracting relation triples to enhance\nresponse generation. Similarly, SUGRE (Kang et al., 2023)\nutilizes Graph Neural Networks (GNN) to embed relevant\nsub-graphs retrieved from the knowledge graph. Knowl-\nedgeGPT (Wang et al., 2023c) generates search queries\nfor Knowledge Bases (KB) in code format and includes\npredefined KB operation functions. Our algorithm can be\nalso viewed as an extension of the RAG concept, which\nencodes historical dialogues into DFA and retrieves similar\ndialogues based on it. Our approach marks a pioneering\nstep in conversational agent research.\nTask-oriented Dialogue System. In this paper, we aim to\nenhance the reliability of LLMs for domain-specific dia-\nlogue generation. A critical subset involves task-oriented\ndialogue (TOD) systems, which are designed to accomplish\nspecific tasks through interactive communication. A no-\ntable line of methods involves leveraging dialogue state\nannotations to train policy-based dialogue generation sys-\ntems (Chen et al., 2019; Ramachandran et al., 2021; Tseng\net al., 2021; Wang et al., 2020b;a; He et al., 2022; Yu et al.,\n2023). Alternatively, some approaches have simplified the\ndesign of TOD systems to the end-to-end (E2E) (Kulh \u00b4anek\net al., 2021; Lee, 2021; Su et al., 2022; Cholakov & Kolev,\n2022a;b; Feng et al., 2022; Bang et al., 2023). Comparing all\nexisting works, our DFA-RAG framework introduces a dis-\ntinct approach without relying on traditional gradient-based\ntraining. This approach offers simplicity and adaptability,\nallowing for easy and efficient adjustments to new scenarios\nin a plug-and-play manner.\nSemantic Router. The Semantic Router (Horsey, 2024;\nazhar, 2024; Avila, 2024; Sisodia, 2024; Hingane, 2024)\nrepresents a recent advancement in LLM technology, par-\nticularly in augmenting the capabilities of chatbots and AI\nassistants through the development of a predefined decision-making layer. This router directs the decision trajectory of\nLLMs by evaluating semantic similarity in each branch and\nselecting the closest match. Typically, similarity is mea-\nsured using cosine distance between the embedding vector\nof inquiries and the potential branches. In this paper, DFA-\nRAG can be seen as an evolution of the semantic router,\noffering several key advantages: a) DFA-RAG navigates the\ndecision-making layer using tags instead of embedding com-\nparison, which enhances interpretability and computational\nefficiency. b) Unlike decision-making layers predefined by\nhumans1, the DFA structure within our framework is lear-\nnale from historical training data, making it more adaptable\nand scalable for real-world applications.\n7. Conclusion\nThis paper proposes the DFA-augmented Large Language\nModel (DFA-RAG), integrating the adaptability of large\nlanguage models with the structured approach of Definite\nFinite Automata.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3973, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b3ba7b87-ccaa-4e9e-93e4-7a56ce10ebcf": {"__data__": {"id_": "b3ba7b87-ccaa-4e9e-93e4-7a56ce10ebcf", "embedding": null, "metadata": {"page_label": "9", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ee28887d-94b6-4892-b0f8-0d3ce82210ab", "node_type": "4", "metadata": {"page_label": "9", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "20c59a918bb7d074524b95b765f367d21589e021549777692717eb73d95130d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dc71cc6f-3115-4644-bfe5-acf79adb2a26", "node_type": "1", "metadata": {"page_label": "9", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "2577bcca99473d9ab7e72ccb9520ff87531100ed0a1c996c7105f9845afce0eb", "class_name": "RelatedNodeInfo"}}, "text": "The DFA structure is learnable from the\ndialogues in the training set. The whole framework ensures\nreliable, contextually appropriate responses, addressing key\nlimitations in current LLM applications, particularly in spe-\ncialized areas like customer service. Extensive experiments\nare conducted to validate DFA-RAG\u2019s effectiveness in gen-\nerating pertinent dialogue content. We hope this pioneering\nresearch can pave the way for more controlled and pre-\ndictable AI conversational agents without sacrificing the\ndynamic nature of LLMs.\nImpact Statement\nThe DFA-RAG framework has the potential to revolutionize\nhow conversational agents are deployed across sectors, in-\ncluding healthcare, customer service, and education, by pro-\nviding more accurate, context-aware, and ethically aligned\ninteractions. This advancement could lead to broader ac-\ncessibility of digital services, enhanced user experiences,\nand greater trust in AI technologies. Beyond the mentioned\nsocietal consequences, we do not anticipate any potentially\nharmful consequences to our work.\nAcknowledgment\nHu is supported by the Wisconsin Alumni Research Founda-\ntion, and by the National Institute Of Biomedical Imaging\nAnd Bioengineering of the National Institutes of Health un-\nder Award Number R01EB033782. The content is solely\nthe responsibility of the authors and does not necessarily\nrepresent the official views of the National Institutes of\nHealth.\n1See Aurelio AI https://www.aurelio.ai/ .\n9", "mimetype": "text/plain", "start_char_idx": 3974, "end_char_idx": 5445, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f1312f1e-7f6e-4e0e-be92-f3c3ccb7a014": {"__data__": {"id_": "f1312f1e-7f6e-4e0e-be92-f3c3ccb7a014", "embedding": null, "metadata": {"page_label": "10", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "42fd54e6-5473-4cbf-9d08-90c03c85bbde", "node_type": "4", "metadata": {"page_label": "10", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "bdb45cbb112bbbe88430b47a64fa5b6f018501a22921298dc53250af5d99957a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3fdff81a-7742-4405-a889-a3b820277638", "node_type": "1", "metadata": {}, "hash": "108dbb1ef1257f8fc0a848b1e534f69168cf01142c95a460e783475502ba86bc", "class_name": "RelatedNodeInfo"}}, "text": "DFA-RAG: Conversational Semantic Router for Large Language Model with Definite Finite Automaton\nReferences\nAbdul-Kader, S. A. and Woods, J. C. Survey on chatbot\ndesign techniques in speech conversation systems. In-\nternational Journal of Advanced Computer Science and\nApplications , 6(7), 2015.\nAdamopoulou, E. and Moussiades, L. Chatbots: History,\ntechnology, and applications. Machine Learning with\nApplications , 2:100006, 2020.\nAvila, D. Semantic router: Enhancing control in\nllm conversations. CodeGPT @ Medium , Jan-\nuary 2024. URL https://blog.codegpt.co/\nsemantic-router-enhancing-control-in-\\\nllm-conversations-68ce905c8d33 .\nAxelbrooke, S. Customer support on twit-\nter (kaggle competition). https://www.\nkaggle.com/datasets/thoughtvector/\ncustomer-support-on-twitter , 2017.\nazhar. Beyond basic chatbots: How semantic router is chang-\ning the game. AI Insights @ Medium , January 2024. URL\nhttps://medium.com/ai-insights-cobet/\nbeyond-basic-chatbots-how-semantic-\\\nrouter-is-changing-the-game-783dd959a32d .\nBang, N., Lee, J., and Koo, M.-W. Task-optimized adapters\nfor an end-to-end task-oriented dialogue system. arXiv\npreprint arXiv:2305.02468 , 2023.\nBickmore, T. W., Utami, D., Matsuyama, R., and Paasche-\nOrlow, M. K. Improving access to online health informa-\ntion with conversational agents: a randomized controlled\nexperiment. Journal of medical Internet research , 18(1):\ne1, 2016.\nBorgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford,\nE., Millican, K., Van Den Driessche, G. B., Lespiau, J.-B.,\nDamoc, B., Clark, A., et al. Improving language models\nby retrieving from trillions of tokens. In International\nconference on machine learning , pp. 2206\u20132240. PMLR,\n2022.\nBudzianowski, P., Wen, T.-H., Tseng, B.-H., Casanueva, I.,\nUltes, S., Ramadan, O., and Ga \u02c7si\u00b4c, M. Multiwoz\u2013a large-\nscale multi-domain wizard-of-oz dataset for task-oriented\ndialogue modelling. arXiv preprint arXiv:1810.00278 ,\n2018.\nChen, W., Chen, J., Qin, P., Yan, X., and Wang, W. Y .\nSemantically conditioned dialog response generation via\nhierarchical disentangled self-attention. arXiv preprint\narXiv:1905.12866 , 2019.Cheng, X., Gao, S., Liu, L., Zhao, D., and Yan, R. Neural\nmachine translation with contrastive translation memo-\nries. In Proceedings of the 2022 Conference on Empiri-\ncal Methods in Natural Language Processing , pp. 3591\u2013\n3601, 2022.\nCheng, X., Luo, D., Chen, X., Liu, L., Zhao, D., and Yan,\nR. Lift yourself up: Retrieval-augmented text genera-\ntion with self memory. arXiv preprint arXiv:2305.02437 ,\n2023.\nCholakov, R. and Kolev, T. Efficient task-oriented dia-\nlogue systems with response selection as an auxiliary\ntask. arXiv preprint arXiv:2208.07097 , 2022a.\nCholakov, R. and Kolev, T. Efficient task-oriented dia-\nlogue systems with response selection as an auxiliary\ntask. arXiv preprint arXiv:2208.07097 , 2022b.\nDing, N., Qin, Y ., Yang, G., Wei, F., Yang, Z., Su, Y ., Hu,\nS., Chen, Y ., Chan, C.-M., Chen, W., et al. Parameter-\nefficient fine-tuning of large-scale pre-trained language\nmodels.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3031, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3fdff81a-7742-4405-a889-a3b820277638": {"__data__": {"id_": "3fdff81a-7742-4405-a889-a3b820277638", "embedding": null, "metadata": {"page_label": "10", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "42fd54e6-5473-4cbf-9d08-90c03c85bbde", "node_type": "4", "metadata": {"page_label": "10", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "bdb45cbb112bbbe88430b47a64fa5b6f018501a22921298dc53250af5d99957a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f1312f1e-7f6e-4e0e-be92-f3c3ccb7a014", "node_type": "1", "metadata": {"page_label": "10", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "49ad5f31978c87fa9818863470c587b1c06d6c3c6f637530857accda56706c7d", "class_name": "RelatedNodeInfo"}}, "text": "Parameter-\nefficient fine-tuning of large-scale pre-trained language\nmodels. Nature Machine Intelligence , 5(3):220\u2013235,\n2023.\nDong, Q., Li, L., Dai, D., Zheng, C., Wu, Z., Chang, B., Sun,\nX., Xu, J., and Sui, Z. A survey for in-context learning.\narXiv preprint arXiv:2301.00234 , 2022.\nDu, X., Sun, Y ., Zhu, J., and Li, Y . Dream the impossible:\nOutlier imagination with diffusion models. Advances in\nNeural Information Processing Systems , 36, 2024.\nDubois, Y ., Li, X., Taori, R., Zhang, T., Gulrajani, I., Ba,\nJ., Guestrin, C., Liang, P., and Hashimoto, T. B. Alpaca-\nfarm: A simulation framework for methods that learn\nfrom human feedback, 2023.\nFeng, Y ., Yang, S., Zhang, S., Zhang, J., Xiong, C., Zhou,\nM., and Wang, H. Fantastic rewards and how to tame\nthem: A case study on reward learning for task-oriented\ndialogue systems. In The Eleventh International Confer-\nence on Learning Representations , 2022.\nGao, Y ., Xiong, Y ., Gao, X., Jia, K., Pan, J., Bi, Y ., Dai, Y .,\nSun, J., and Wang, H. Retrieval-augmented generation\nfor large language models: A survey. arXiv preprint\narXiv:2312.10997 , 2023.\nGhosal, S. S., Sun, Y ., and Li, Y . How to overcome curse-\nof-dimensionality for out-of-distribution detection? In\nProceedings of the AAAI Conference on Artificial Intelli-\ngence , volume 38, pp. 19849\u201319857, 2024.\nHe, W., Dai, Y ., Zheng, Y ., Wu, Y ., Cao, Z., Liu, D., Jiang,\nP., Yang, M., Huang, F., Si, L., et al. Galaxy: A gen-\nerative pre-trained model for task-oriented dialog with\nsemi-supervised learning and explicit policy injection. In\n10", "mimetype": "text/plain", "start_char_idx": 2955, "end_char_idx": 4521, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f54ff78d-a4a3-4460-a49a-41f88a058079": {"__data__": {"id_": "f54ff78d-a4a3-4460-a49a-41f88a058079", "embedding": null, "metadata": {"page_label": "11", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a275cc4e-a022-498c-a869-6013536c367f", "node_type": "4", "metadata": {"page_label": "11", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "5daf3ce4c631cf2ebafde77fe5af84b555403231a90f29080afbf2cffe82f338", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "003a9261-5643-4655-afab-e25cb6260094", "node_type": "1", "metadata": {}, "hash": "d71e6edc7cd2b8afe31263c806b8083b2ddbb674478c2697af88f1f84f782721", "class_name": "RelatedNodeInfo"}}, "text": "DFA-RAG: Conversational Semantic Router for Large Language Model with Definite Finite Automaton\nProceedings of the AAAI conference on artificial intelli-\ngence , volume 36, pp. 10749\u201310757, 2022.\nHingane, A. Llm apps: Why you must know semantic\nrouter in 2024: Part 1. Medium , January 2024. URL\nhttps://medium.com/@learn-simplified/\nllm-apps-why-you-must-know-semantic-\\\nrouter-in-2024-part-1-bfbda81374c5 .\nHorsey, J. Semantic router superfast decision\nlayer for llms and ai agents, January 2024.\nURL https://www.geeky-gadgets.com/\nsemantic-router-superfast-decision-\\\nlayer-for-llms-and-ai-agents/ .\nHussain, S., Ameri Sianaki, O., and Ababneh, N. A sur-\nvey on conversational agents/chatbots classification and\ndesign techniques. In Web, Artificial Intelligence and\nNetwork Applications: Proceedings of the Workshops of\nthe 33rd International Conference on Advanced Informa-\ntion Networking and Applications (WAINA-2019) 33 , pp.\n946\u2013956. Springer, 2019.\nJain, M., Kumar, P., Kota, R., and Patel, S. N. Evaluating\nand informing the design of chatbots. In Proceedings of\nthe 2018 designing interactive systems conference , pp.\n895\u2013906, 2018.\nJanssen, A., Passlick, J., Rodr \u00b4\u0131guez Cardona, D., and Breit-\nner, M. H. Virtual assistance in any context: A taxonomy\nof design elements for domain-specific chatbots. Business\n& Information Systems Engineering , 62:211\u2013225, 2020.\nKang, M., Kwak, J. M., Baek, J., and Hwang, S. J.\nKnowledge graph-augmented language models for\nknowledge-grounded dialogue generation. arXiv preprint\narXiv:2305.18846 , 2023.\nKhattab, O., Santhanam, K., Li, X. L., Hall, D., Liang, P.,\nPotts, C., and Zaharia, M. Demonstrate-search-predict:\nComposing retrieval and language models for knowledge-\nintensive nlp. arXiv preprint arXiv:2212.14024 , 2022.\nKleene, S. C., Shannon, C. E., and McCarthy, J. Automata\nstudies. Princeton, NJ , 1956.\nKulh \u00b4anek, J., Hude \u02c7cek, V ., Nekvinda, T., and Du \u02c7sek, O.\nAugpt: Auxiliary tasks and data augmentation for end-\nto-end dialogue with pre-trained language models. arXiv\npreprint arXiv:2102.05126 , 2021.\nLee, Y . Improving end-to-end task-oriented dialog system\nwith a simple auxiliary task. In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2021 , pp.\n1296\u20131303, 2021.\nLewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V .,\nGoyal, N., K \u00a8uttler, H., Lewis, M., Yih, W.-t., Rockt \u00a8aschel,T., et al. Retrieval-augmented generation for knowledge-\nintensive nlp tasks. Advances in Neural Information Pro-\ncessing Systems , 33:9459\u20139474, 2020.\nLi, X. and Qiu, X. Finding support examples for in-context\nlearning. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2023 , pp. 6219\u20136235, 2023.\nMedeiros, L., Gerritsen, C., and Bosse, T. Towards hu-\nmanlike chatbots helping users cope with stressful situ-\nations. In Computational Collective Intelligence: 11th\nInternational Conference, ICCCI 2019, Hendaye, France,\nSeptember 4\u20136, 2019, Proceedings, Part I 11 , pp. 232\u2013\n243. Springer, 2019.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3000, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "003a9261-5643-4655-afab-e25cb6260094": {"__data__": {"id_": "003a9261-5643-4655-afab-e25cb6260094", "embedding": null, "metadata": {"page_label": "11", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a275cc4e-a022-498c-a869-6013536c367f", "node_type": "4", "metadata": {"page_label": "11", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "5daf3ce4c631cf2ebafde77fe5af84b555403231a90f29080afbf2cffe82f338", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f54ff78d-a4a3-4460-a49a-41f88a058079", "node_type": "1", "metadata": {"page_label": "11", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "1ad06ffac69d235887d4d2a2d3c36dbece10f97616e44b4aa8a777f3fdf378ed", "class_name": "RelatedNodeInfo"}}, "text": "232\u2013\n243. Springer, 2019.\nMing, Y ., Cai, Z., Gu, J., Sun, Y ., Li, W., and Li, Y . Delving\ninto out-of-distribution detection with vision-language\nrepresentations. Advances in neural information process-\ning systems , 35:35087\u201335102, 2022a.\nMing, Y ., Sun, Y ., Dia, O., and Li, Y . How to exploit hyper-\nspherical embeddings for out-of-distribution detection?\nInThe Eleventh International Conference on Learning\nRepresentations , 2022b.\nMinsky, M. Some universal elements for finite automata.\nAutomata studies , 34:117\u2013128, 1956.\nModarressi, A., Imani, A., Fayyaz, M., and Sch \u00a8utze, H. Ret-\nllm: Towards a general read-write memory for large lan-\nguage models. arXiv preprint arXiv:2305.14322 , 2023.\nOpenAI. Gpt-4 technical report. 2023.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\net al. Training language models to follow instructions\nwith human feedback. Advances in Neural Information\nProcessing Systems , 35:27730\u201327744, 2022.\nRamachandran, G. S., Hashimoto, K., and Xiong, C. Causal-\naware safe policy improvement for task-oriented dialogue.\narXiv preprint arXiv:2103.06370 , 2021.\nSafi, Z., Abd-Alrazaq, A., Khalifa, M., and Househ, M.\nTechnical aspects of developing chatbots for medical ap-\nplications: scoping review. Journal of medical Internet\nresearch , 22(12):e19127, 2020.\nSelvi, J. Exploring overfitting risks in large language mod-\nels. https://research.nccgroup.com/2023/05/22/exploring-\noverfitting-risks-in-large-language-models/, May 2023.\nShuster, K., Poff, S., Chen, M., Kiela, D., and Weston, J.\nRetrieval augmentation reduces hallucination in conver-\nsation. In Findings of the Association for Computational\nLinguistics: EMNLP 2021 , pp. 3784\u20133803, 2021.\n11", "mimetype": "text/plain", "start_char_idx": 2975, "end_char_idx": 4734, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3948973c-04a3-4443-a96d-44b243f042e1": {"__data__": {"id_": "3948973c-04a3-4443-a96d-44b243f042e1", "embedding": null, "metadata": {"page_label": "12", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d5dfac96-c94e-48b7-927d-ace75146706f", "node_type": "4", "metadata": {"page_label": "12", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "37f9ef85f40e3e94d2c1e0a503813b6aca4f6f55cb90acf6875353a0cebb8015", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "48de820b-9318-4dcc-91db-f0a6c9f8847b", "node_type": "1", "metadata": {}, "hash": "3c8f556790a1759317c8589f410cc89fa3d1d7fe50489f11366fd6df40a9bcbd", "class_name": "RelatedNodeInfo"}}, "text": "DFA-RAG: Conversational Semantic Router for Large Language Model with Definite Finite Automaton\nSisodia, Y . Stop chat-gpt from going rogue in produc-\ntion with semantic router. Medium , January 2024.\nURL https://medium.com/@scholarly360/\nstop-chat-gpt-from-going-rogue-in-\\\nproduction-with-semantic-router-\\\n937a4768ae19 .\nSu, Y ., Shu, L., Mansimov, E., Gupta, A., Cai, D., Lai, Y .-A.,\nand Zhang, Y . Multi-task pre-training for plug-and-play\ntask-oriented dialogue system. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pp. 4661\u20134676,\n2022.\nSun, Y . Detecting and Learning Out-of-distribution Data in\nthe Open-world: Algorithm and Theory . The University\nof Wisconsin-Madison, 2023.\nSun, Y ., Shi, Z., Liang, Y ., and Li, Y . When and how\ndoes known class help discover unknown ones? provable\nunderstanding through spectral analysis. arXiv preprint\narXiv:2308.05017 , 2023.\nSun, Y ., Shi, Z., and Li, Y . A graph-theoretic framework\nfor understanding open-world semi-supervised learning.\nAdvances in Neural Information Processing Systems , 36,\n2024.\nTseng, B.-H., Dai, Y ., Kreyssig, F., and Byrne, B. Transfer-\nable dialogue systems and user simulators. arXiv preprint\narXiv:2107.11904 , 2021.\nWang, B., Chen, W., Pei, H., Xie, C., Kang, M., Zhang,\nC., Xu, C., Xiong, Z., Dutta, R., Schaeffer, R., et al.\nDecodingtrust: A comprehensive assessment of trustwor-\nthiness in gpt models. Advances in Neural Information\nProcessing Systems , 2023a.\nWang, J., Zhang, Y ., Kim, T.-K., and Gu, Y . Modelling hier-\narchical structure between dialogue policy and natural lan-\nguage generator with option framework for task-oriented\ndialogue system. arXiv preprint arXiv:2006.06814 ,\n2020a.\nWang, K., Tian, J., Wang, R., Quan, X., and Yu, J. Multi-\ndomain dialogue acts and response co-generation. arXiv\npreprint arXiv:2004.12363 , 2020b.\nWang, S., Zhao, Z., Ouyang, X., Wang, Q., and Shen, D.\nChatcad: Interactive computer-aided diagnosis on med-\nical image using large language models. arXiv preprint\narXiv:2302.07257 , 2023b.\nWang, X., Yang, Q., Qiu, Y ., Liang, J., He, Q., Gu, Z., Xiao,\nY ., and Wang, W. Knowledgpt: Enhancing large language\nmodels with retrieval and storage access on knowledge\nbases. arXiv preprint arXiv:2308.11761 , 2023c.Yang, J., Wang, P., Zou, D., Zhou, Z., Ding, K., Peng, W.,\nWang, H., Chen, G., Li, B., Sun, Y ., et al. Openood:\nBenchmarking generalized out-of-distribution detection.\nAdvances in Neural Information Processing Systems , 35:\n32598\u201332611, 2022.\nYasunaga, M., Aghajanyan, A., Shi, W., James, R.,\nLeskovec, J., Liang, P., Lewis, M., Zettlemoyer, L., and\nYih, W.-t. Retrieval-augmented multimodal language\nmodeling. arXiv preprint arXiv:2211.12561 , 2022.\nYu, X., Wu, Q., Qian, K., and Yu, Z. Krls: Improving end-\nto-end response generation in task oriented dialog with\nreinforced keywords learning. In Proceedings of the 2023\nConference on Empirical Methods in Natural Language\nProcessing , pp.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3001, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "48de820b-9318-4dcc-91db-f0a6c9f8847b": {"__data__": {"id_": "48de820b-9318-4dcc-91db-f0a6c9f8847b", "embedding": null, "metadata": {"page_label": "12", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d5dfac96-c94e-48b7-927d-ace75146706f", "node_type": "4", "metadata": {"page_label": "12", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "37f9ef85f40e3e94d2c1e0a503813b6aca4f6f55cb90acf6875353a0cebb8015", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3948973c-04a3-4443-a96d-44b243f042e1", "node_type": "1", "metadata": {"page_label": "12", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "846249fe626e835617a69ed2b44e299cce0e6012ee8508fe1369f35e842407fe", "class_name": "RelatedNodeInfo"}}, "text": "12338\u201312358, 2023.\nZhang, J., Yang, J., Wang, P., Wang, H., Lin, Y ., Zhang, H.,\nSun, Y ., Du, X., Zhou, K., Zhang, W., et al. Openood v1.\n5: Enhanced benchmark for out-of-distribution detection.\narXiv preprint arXiv:2306.09301 , 2023.\nZheng, L., Chiang, W.-L., Sheng, Y ., Zhuang, S., Wu, Z.,\nZhuang, Y ., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging\nllm-as-a-judge with mt-bench and chatbot arena. arXiv\npreprint arXiv:2306.05685 , 2023.\n12", "mimetype": "text/plain", "start_char_idx": 3002, "end_char_idx": 3449, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4304dee3-e26d-4278-a690-a35b8e99d5d3": {"__data__": {"id_": "4304dee3-e26d-4278-a690-a35b8e99d5d3", "embedding": null, "metadata": {"page_label": "13", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8d94c55a-8566-4f87-a584-9786cc35f753", "node_type": "4", "metadata": {"page_label": "13", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "902a0f1e310ffbea9ffca9f8ce1375ee0fae7163350fddca6b339d8e2652832f", "class_name": "RelatedNodeInfo"}}, "text": "DFA-RAG: Conversational Semantic Router for Large Language Model with Definite Finite Automaton\nA. Prompts\nPrompts used for extracting tags. This prompt template is used in Section 3.2.1 to make sure the resulting tags are both\nconcise and relevant to the conversational content.\n1# Task Description\n2\n3You are helping me compress the following dialog with customer service into the following\nform:\n4\n5<id> <User/System>: <compressed phrase>\n6\n7You will have to follow several principles:\n81. Please use words as few as possible, ideally no more than 3 words.\n92. The summarization needs to focus on the actual events/issues/queries/solutions.\n10\n11\n12# Example\n13\n14Input:\n15\"0 User: What is going on with my keyboard... fix it\"\n16Output:\n17\"0 User: #keyboard #issue\"\nPrompts used for generating response. This prompt template is used in Section 3.3 to generate the response given the\nexample dialogues.\n1# Task Description\n2You are a helpful service agent. Please help me fill in the system response in a dialogue.\n3Please note that key information is encoded in the dialogue.\n4\n5The dialogue is with the format:\n6\n7[ID] [USER/SYSTEM]: [UTTERANCE]\n8\n9Here is a list of related example dialogues you can use for reference.\n10\n11{examples}\n12\n13# Remarks:\n14\n151. Please directly generate the completed dialogue according to the format in the example.\n162. ( **IMPORTANT **) Please make sure the generated utterance ID is consistent with the\noriginal input!\nPrompts used for evaluation. This prompt template is used in Section 4.1 to evaluate the response quality and win rate\ncalculation. This prompt is modified from AlpacaEval (Dubois et al., 2023).\n1\n2I\u2019ll provide you with task prompts given to these models and their corresponding outputs.\n3 Your task is to assess these responses, and select the model that produces the output\nthat is most smooth and consistent with the ground truth dialogue.\n4 Please note that it is very important for model to provide response in a **similar\nstyle and content **.\n5\n6 ## Instruction\n7\n8 {{\n9 \"instruction\": \"\"\"Please act as a helpful customer service agent and complete the\nfollowing dialogue: \"\"\",\n10 \"input\":\n11 \"\"\"\n12 {task_input}\n13", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2180, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6f0fe2cc-ca36-493b-bd8e-ea9b218518bd": {"__data__": {"id_": "6f0fe2cc-ca36-493b-bd8e-ea9b218518bd", "embedding": null, "metadata": {"page_label": "14", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f40f0e73-aa49-467f-b26e-1b77ae3a1f79", "node_type": "4", "metadata": {"page_label": "14", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "fa7044440a5edfdbce9feb759b9258367e5a52745315fca2839ccb5cba4cc417", "class_name": "RelatedNodeInfo"}}, "text": "DFA-RAG: Conversational Semantic Router for Large Language Model with Definite Finite Automaton\n13 \"\"\",\n14 \"ground truth answer\": \"\"\"\n15 {raw_diag}\n16 \"\"\"\n17 \"\"\"\n18 }}\n19\n20 ## Model Outputs\n21\n22 Here are the unordered outputs from the models. Each output is associated with a\nspecific model, identified by a unique model identifier.\n23\n24 {{\n25 {{\n26 \"model_identifier\": \"m\",\n27 \"output\": \"\"\"\n28 {pred_cmp_diag}\n29 \"\"\"\n30 }},\n31 {{\n32 \"model_identifier\": \"M\",\n33 \"output\": \"\"\"\n34 {pred_diag}\n35 \"\"\"\n36 }}\n37 }}\n38\n39 ## Task\n40\n41 Evaluate the models based on the quality and relevance of their outputs, and select\nthe model that generated the best output.\n42 Answer by first providing a concise explanation and then end your answer by providing\nthe model identifier of the best output.\n43 We will use the last character of your output \u2018output[-1]\u2018 as the name of the best\nmodel, so make sure you finish with the token of the model identifiers and nothing\nelse: \u2018m\u2018 or \u2018M\u2018 (no quotes, no dots, no backticks, no new lines, ...).\n44 For example:\n45\n46 ### Concise explanation\n47 ...some text...\n48\n49 ### Which is best, m or M?\n50 M\n51\n52 Now is your turn.\n53\n54 ## Your answer: \"Concise explanation\" followed by \"Which is best, m or M?\"\nB. Experiment Details\nIn this section, we delve into the specifics of implementing the DFA learning process from training data, as outlined in\nSection 3.2, and the intricacies of conversation generation, discussed in Section 3.3.\nA threshold for determining the \u201csplittable\u201d states . Figure 3 illustrates the algorithm for constructing the DFA-tree.\nHowever, the practical application of this algorithm involves several nuanced considerations. One key aspect is the handling\nof nodes/states that correspond to only a limited number of dialogues in the training dataset. Constructing a sub-tree\nunder such nodes/states may not yield significant insights due to their limited number of tracked dialogues. Therefore, the\ntree-building function, as depicted in Figure 3, is only pursued if the number of tracked dialogue IDs at a node/state exceeds\na certain threshold: |I(q)|> \u03c4. For our experiments, we have set this threshold, \u03c4, to 5.\nState-merging threshold . In Section 3.2.3, we illustrate that once the similarity score is calculated, we set a threshold \u03bbto\ndetermine which node pairs should be merged. In practice, we set \u03bb= 0.1and by observation, 5%of nodes get merged\n14", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2415, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d0581db3-4277-4801-920d-47f32fff28f7": {"__data__": {"id_": "d0581db3-4277-4801-920d-47f32fff28f7", "embedding": null, "metadata": {"page_label": "15", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2ac0cc0d-5faa-4e16-82a4-1f3a303d7006", "node_type": "4", "metadata": {"page_label": "15", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "96bb9acf25eb95830ec589f00482609de5e4ba345d8961112381ddf9914f5fef", "class_name": "RelatedNodeInfo"}}, "text": "DFA-RAG: Conversational Semantic Router for Large Language Model with Definite Finite Automaton\nwith the threshold set.\nExamples limits in In-Context Learning (ICL) . In alignment with the practice in in-context learning (Li & Qiu, 2023;\nDong et al., 2022), we limit the maximum number of examples used in in-context learning to 5. This means that if the\nnumber of tracked dialogue IDs for a state, denoted as |I(q)|, exceeds 5, we randomly select 5 dialogues from |I(q)|for\nfurther processing.\nC. Dialogue Generation Results Demonstrations\nIn this section, we conduct a qualitative analysis of dialogue generation outcomes using the baseline algorithms outlined\nin Table 1. The dialogues are derived from the AirbnbHelp dataset, with all baseline models utilizing GPT-3.5 as the\nunderlying large language model (LLM) for generation. Our objective is to compare each model\u2019s output with the ground\ntruth dialogue from the dataset, aiming for generated dialogues that closely mirror the original interactions. Additionally, we\ninclude insights from a GPT-4 evaluator to shed light on the relative performance of each model.\nC.1. Ground Truth Dialogues\n1.User : I just received a mail asking me to give feedback on a trip that I didn\u2019t make. Error at your end or has\nmy account been hacked?\n2.System : Hey Nick, thank you for bringing this to our attention. We\u2019ve escalated a case to the appropriate\nteam and a Case Manager will follow up with you for further assistance. We appreciate your patience in the\nmeantime.\n3.User : I presume you got my message via the Help Center contact form?\n4.System : Yes, we\u2019ve also noticed that your case manager has just sent you an email with all the instructions to\nrecover your account. Please let us know if you need further assistance. Thanks.\n5.User : Well, I changed my password, and my account seems to be working. I canceled the trip and it said I\nwould get a refund, but whoever made the trip didn\u2019t use my credit card, so I didn\u2019t pay!\n6.System : Hey Nick, we\u2019ve looked into it and everything looks good. You shouldn\u2019t have to upload your ID\nagain, as we see your account is still fully verified. Please let us know if there\u2019s anything else we can do for\nyou. Thank you.\nC.2. GPT-3.5 Generation Results\nBelow, we display the dialogue completions generated by GPT-3.5, without using any additional prompts:\n15", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2352, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bd207ab8-21a7-4063-b9f1-7b3e98043d33": {"__data__": {"id_": "bd207ab8-21a7-4063-b9f1-7b3e98043d33", "embedding": null, "metadata": {"page_label": "16", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a31fcd72-9352-4e2a-a6ff-4a35fa85a8bf", "node_type": "4", "metadata": {"page_label": "16", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "aece9834d4ed02f46c3f57c41581e8449934d3f831e9903f4a0fc4fdb19f478c", "class_name": "RelatedNodeInfo"}}, "text": "DFA-RAG: Conversational Semantic Router for Large Language Model with Definite Finite Automaton\n1.User : I just received a mail asking me to give feedback on a trip that I didn\u2019t make. Error at your end or has\nmy account been hacked?\n2.System : It seems like there might have been some misunderstanding. I\u2019ll need to investigate this further.\nCould you please provide me with the email address that received the feedback request?\n(Comment from GPT-4 : It asks for the email address from the user, although presumably, the system should already have\nthis information since the user received an unwanted email from the service.)\n3.User : I presume you got my message via the Help Center contact form?\n4.System : Yes, I received your message through the Help Center contact form. Thank you for reaching out to us that way.\n5.User : Well, I changed my password, and my account seems to be working. I canceled the trip and it said I would get a\nrefund, but whoever made the trip didn\u2019t use my credit card, so I didn\u2019t pay!\n6.System : I\u2019m glad to hear that you changed your password and that your account is working. Our team will investigate the\nunauthorized trip and ensure that you are not charged for it. Rest assured that we will take care of the refund process for\nyou.\nThese results, while informative, reveal certain limitations in capturing the nuances of the original conversation, as will be\ndiscussed in the evaluator\u2019s comments.\nC.3. DFA-RAG Generation Results (Ours)\nWe next present results from the DFA-RAG approach. The GPT-4 evaluator offers more favorable comments for these\noutputs, highlighting an improved alignment with the ground truth dialogue:\n1.User : I just received a mail asking me to give feedback on a trip that I didn\u2019t make. Error at your end or has\nmy account been hacked?\n2.System : Hi there, it sounds like there may have been some confusion with your account. We\u2019ll investigate\nthis further and get back to you with more information. Thank you for bringing this to our attention.\n3.User : I presume you got my message via the Help Center contact form?\n4.System : Our team is currently reviewing your message and will provide you with an update as soon as\npossible. Thank you for your understanding.\n5.User : Well, I changed my password, and my account seems to be working. I canceled the trip and it said I\nwould get a refund, but whoever made the trip didn\u2019t use my credit card, so I didn\u2019t pay!\n6.System : I\u2019ll ensure that your account is thoroughly checked to confirm its security and functionality. We\nappreciate your proactive steps, and we\u2019ll keep you updated on any developments.\n(Comment from GPT-4 : This model does not ask for additional information that it should already have and instead\nfocuses on confirming that the issue will be investigated and the user will be updated. This is closer to the \u201cground truth\u201d\nresponse mentioned in the instruction, where the system acknowledges the receipt of the message and provides assurance\nwithout requesting information it should already possess. In addition, this model uses language that is more in line with the\nprofessional customer service manner shown in the ground truth dialogue, such as thanking the user multiple times and\nindicating appreciation of the user\u2019s proactive measures.)\nC.4. RAG-Based Generation Results\nResults from the RAG (Retrieval-Augmented Generation) model are also presented. This model retrieves dialogues that\nclosely match the context in the embedding space, using these selections as prompts for generation. However, the generated\nresponses often inaccurately focus on requesting the user\u2019s email, failing to directly address the user\u2019s initial request:\n16", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3676, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "300cb555-aab8-40dc-b8f6-c467e2e905f9": {"__data__": {"id_": "300cb555-aab8-40dc-b8f6-c467e2e905f9", "embedding": null, "metadata": {"page_label": "17", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a9025ae8-c380-403f-8f01-abf432feb81e", "node_type": "4", "metadata": {"page_label": "17", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "0cf85f647dee1d46a8aca5abebbef8042fd8ad29d719590b63d1dc897df68155", "class_name": "RelatedNodeInfo"}}, "text": "DFA-RAG: Conversational Semantic Router for Large Language Model with Definite Finite Automaton\n1.User : I just received a mail asking me to give feedback on a trip that I didn\u2019t make. Error at your end or has\nmy account been hacked?\n2.System : Hi there, we\u2019re sorry to hear about this. Can you please DM us with your account email so we can\nlook into this for you and provide further assistance? Thank you.\n3.User : I presume you got my message via the Help Center contact form?\n4.System : Thanks for confirming that. We\u2019ll review your account and ensure everything is in order. If you have\nany further concerns, please don\u2019t hesitate to reach out to us.\n5.User : Well, I changed my password, and my account seems to be working. I canceled the trip and it said I\nwould get a refund, but whoever made the trip didn\u2019t use my credit card, so I didn\u2019t pay!\n6.System : We understand your concern and will definitely look into it for you. Please DM us your email\naddress so we can verify your account and address any potential security issues. Thank you.\n(Comment from GPT-4 : It did not address the final user query. Furthermore, the model started well by asking for a Direct\nMessage (DM) for privacy, it did not fully reassure the user regarding the data breach or provide clear next steps concerning\nthe unauthorized trip.)\nC.5. RandSample Generation Results\nFinally, we discuss outcomes from the RandSample method, which randomly selects dialogues from the training dataset\nfor prompt generation. This approach sometimes leads to generated dialogues that contain hallucinated content, such as\nmismatched usernames, illustrating the pitfalls of random selection:\n1.User : I just received a mail asking me to give feedback on a trip that I didn\u2019t make. Error at your end or has\nmy account been hacked?\n2.System : We appreciate you reaching out, Simona. It seems there may have been a mix-up. Please DM us\nyour email address so we can look into this further for you.\n3.User : I presume you got my message via the Help Center contact form?\n4.System : We understand your concern, Simona. We assure you that we will investigate this and get back to\nyou with an update.\n5.User : Well, I changed my password, and my account seems to be working. I canceled the trip and it said I\nwould get a refund, but whoever made the trip didn\u2019t use my credit card, so I didn\u2019t pay!\n6.System : We\u2019ll definitely look into this for you, Simona. Please DM us your email address so we can check\nthe status of your account and the refund for the canceled trip. Thank you.\n(Comment from GPT-4 : This model consistently addresses the user by the name \u201cSimona,\u201d which does not align with the\nground truth dialogue where the user is referred to as Nick. )\nEach of these methodologies offers unique insights into the challenges and opportunities present in dialogue generation.\nThrough the lens of the GPT-4 evaluator\u2019s comments, we gain a nuanced understanding of how each model performs relative\nto the complexities of real-world conversation dynamics.\nD. More DFA Results Demonstrations\nWe include the constructed DFA for each dataset in Figure 6, Figure 7, Figure 8, Figure 9, Figure 10, Figure 11, we have\nconstrained the maximum depth to four levels due to spatial constraints, and have chosen to display only those nodes\nthat correspond to a minimum of ten dialogues in the training set, as indicated by |I(q)|>10. It\u2019s important to note\nthat, owing to the limitations of our tree-like visualization tool, the \u2018looping\u2019 structures typical of DFAs, characterized by\n17", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3541, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b11f282c-5ead-4150-b193-9ce037de8ff3": {"__data__": {"id_": "b11f282c-5ead-4150-b193-9ce037de8ff3", "embedding": null, "metadata": {"page_label": "18", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c48cb58e-c7c4-45aa-8210-a877662d6ec5", "node_type": "4", "metadata": {"page_label": "18", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "3ca3237fe04a791e9f34cea84bcfee420550efa82ad7c8dbdd7f4d9e9c76706b", "class_name": "RelatedNodeInfo"}}, "text": "DFA-RAG: Conversational Semantic Router for Large Language Model with Definite Finite Automaton\nnodes with multiple incoming edges, are not depicted here. Additionally, for clarity in this visualization, tags are shown\nattached to the nodes; however, in the actual DFA model, these tags are associated with the edges leading to each state/node.\nThis visualization is intended to provide an overarching view of the DFA\u2019s structure. Readers should be mindful of these\ndistinctions to accurately understand and interpret the DFA results.\n18", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 537, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6f00e998-e4a7-4a34-978e-421358819c3a": {"__data__": {"id_": "6f00e998-e4a7-4a34-978e-421358819c3a", "embedding": null, "metadata": {"page_label": "19", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e2f11d09-4e6e-4ee8-aa84-500728571ef9", "node_type": "4", "metadata": {"page_label": "19", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "201b58a8163ff21b98381363a7b02e479dd75d3ed16898f4613a6e9272b638ca", "class_name": "RelatedNodeInfo"}}, "text": "DFA-RAG: Conversational Semantic Router for Large Language Model with Definite Finite Automaton\nFigure 6. DFA Results Demonstration for AmazonHelp .\n19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 151, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "518693d7-8057-4a6c-9771-e1a97399f2da": {"__data__": {"id_": "518693d7-8057-4a6c-9771-e1a97399f2da", "embedding": null, "metadata": {"page_label": "20", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f735b33d-be49-47f2-a8e4-614d2963e5d5", "node_type": "4", "metadata": {"page_label": "20", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "85ed81d1fd6a7b21a07278b37c1c33e678f97c6651f55a61c3fe49de85503cf6", "class_name": "RelatedNodeInfo"}}, "text": "DFA-RAG: Conversational Semantic Router for Large Language Model with Definite Finite Automaton\nFigure 7. DFA Results Demonstration for DeltaSupport .\n20", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 153, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4dba1649-ec15-489e-9214-d51da906145e": {"__data__": {"id_": "4dba1649-ec15-489e-9214-d51da906145e", "embedding": null, "metadata": {"page_label": "21", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b37bbca1-ad20-4b10-a125-5703360ec665", "node_type": "4", "metadata": {"page_label": "21", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "8113e8b11fdfe672b0245a7d291c946d8f43a63838b7d2f09b01f7d54dbe3ed7", "class_name": "RelatedNodeInfo"}}, "text": "DFA-RAG: Conversational Semantic Router for Large Language Model with Definite Finite Automaton\nFigure 8. DFA Results Demonstration for AskPlayStation .\n21", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 155, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "93d8512f-5257-452f-bce9-5eea5f8c894d": {"__data__": {"id_": "93d8512f-5257-452f-bce9-5eea5f8c894d", "embedding": null, "metadata": {"page_label": "22", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f6bd37e8-19bd-4616-981e-541d875446b0", "node_type": "4", "metadata": {"page_label": "22", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "d7ba8a07ee74012dc03cd0f6d6f116bad3e2ce117d4f55a704121c9f0ada835c", "class_name": "RelatedNodeInfo"}}, "text": "DFA-RAG: Conversational Semantic Router for Large Language Model with Definite Finite Automaton\nFigure 9. DFA Results Demonstration for AirbnbHelp .\n22", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 151, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3b08562d-bf8e-4707-b3aa-4a677729d21f": {"__data__": {"id_": "3b08562d-bf8e-4707-b3aa-4a677729d21f", "embedding": null, "metadata": {"page_label": "23", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d30725b2-8917-4b80-aa26-b6ab1d5d4328", "node_type": "4", "metadata": {"page_label": "23", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "1ed2c91480ca076f15043ad79b4b9f86b475f3065957e611509177b95958ee16", "class_name": "RelatedNodeInfo"}}, "text": "DFA-RAG: Conversational Semantic Router for Large Language Model with Definite Finite Automaton\nFigure 10. DFA Results Demonstration for NikeSupoort .\n23", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 153, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "63819359-9937-491d-a1b3-54cdae85df43": {"__data__": {"id_": "63819359-9937-491d-a1b3-54cdae85df43", "embedding": null, "metadata": {"page_label": "24", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9366bf78-585a-487f-919f-aaa1e1c8a2d7", "node_type": "4", "metadata": {"page_label": "24", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "0226df85e51cffd34e115441f77690a3bd575f984f9c0af13e2a561ad4d7049c", "class_name": "RelatedNodeInfo"}}, "text": "DFA-RAG: Conversational Semantic Router for Large Language Model with Definite Finite Automaton\nFigure 11. DFA Results Demonstration for CambridgeInfo /MultiWOZ . We extend the depth limitation to eight in the demo due to\nthe complicated branching in this demo.\n24", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 264, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6ae1976-9d75-4bad-aa02-3dffbb741229": {"__data__": {"id_": "e6ae1976-9d75-4bad-aa02-3dffbb741229", "embedding": null, "metadata": {"page_label": "1", "file_name": "MachineTranslationWithAttention.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\MachineTranslationWithAttention.pdf", "file_type": "application/pdf", "file_size": 481968, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c85e4e9f-d83b-429e-925d-502f1e8647e4", "node_type": "4", "metadata": {"page_label": "1", "file_name": "MachineTranslationWithAttention.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\MachineTranslationWithAttention.pdf", "file_type": "application/pdf", "file_size": 481968, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "c87d8f6d5162ca151bd28a4a4d4694a0df0de1e1de28b80a5f36b0bd65292750", "class_name": "RelatedNodeInfo"}}, "text": "See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/355917108\nNeu ral Machine T ranslation with Attention\nTechnic al R eport  \u00b7 August 2021\nDOI: 10.13140/RG.2.2.29381.37607/1\nCITATION\n1READS\n6,186\n2 author s:\nMohammad W asil Saleem\nUniv ersit\u00e4t P otsdam\n4 PUBLICA TIONS \u00a0\u00a0\u00a02 CITATIONS \u00a0\u00a0\u00a0\nSEE PROFILE\nSandeep Upr ety\nUniv ersit\u00e4t P otsdam\n1 PUBLICA TION \u00a0\u00a0\u00a01 CITATION \u00a0\u00a0\u00a0\nSEE PROFILE\nAll c ontent f ollo wing this p age was uplo aded b y Sandeep Upr ety on 05 No vember 2021.\nThe user has r equest ed enhanc ement of the do wnlo aded file.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 605, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6157bec3-85a2-482b-9014-f4e118e443fa": {"__data__": {"id_": "6157bec3-85a2-482b-9014-f4e118e443fa", "embedding": null, "metadata": {"page_label": "2", "file_name": "MachineTranslationWithAttention.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\MachineTranslationWithAttention.pdf", "file_type": "application/pdf", "file_size": 481968, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "89a4565d-5b89-4b6a-9758-40eb6b17b1bb", "node_type": "4", "metadata": {"page_label": "2", "file_name": "MachineTranslationWithAttention.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\MachineTranslationWithAttention.pdf", "file_type": "application/pdf", "file_size": 481968, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "48c1123e96681284cfa4e202cea2ae4b80e11763a7ec496a5960549025f2ee0c", "class_name": "RelatedNodeInfo"}}, "text": "Neural Machine Translation with Attention\nMohammad Wasil Saleem\nMatrikel-Nr.: 805779\nUniversit \u00a8at Potsdam\nsaleem1@uni-potsdam.deSandeep Uprety\nMatrikel-Nr. 804982\nUniversit \u00a8at Potsdam\nuprety@uni-potsdam.de\nAbstract\nIn recent years, the success achieved\nthrough neural machine translation has\nmade it mainstream in machine translation\nsystems. In this work, encoder-decoder\nwith attention system based on \u201dNeural\nMachine Translation by Jointly Learning\nto Align and Translate\u201d by Bahdanau et al.\n(2014) has been used to accomplish the\nMachine Translation between English and\nSpanish Language which has not seen\nmuch research work done as compared\nto other languages such as German and\nFrench. We aim to demonstrate the re-\nsults similar to the breakthrough paper on\nwhich our work is based on. We achieved\na BLEU score of 25.37, which was close\nenough to what Bahdanau et al. (2014)\nachieved in their work.\n1 Introduction\nMachine Translation (MT) is the task of translat-\ning text without human assistance while preserv-\ning the meaning of input text. The early approach\nto machine translation relied heavily on hand-\ncrafted translation rules and linguistic knowledge.\nStarted in early around 1950s, unlike rule-based\nmachine translation, Statistical machine transla-\ntion (SMT) generated translations based on statis-\ntical models whose parameters are derived from\nthe analysis of bilingual text corpora (Koehn et al.,\n2003). Though reliable, for SMT, it can be hard\nto \ufb01nd content for obscure languages and is less\nsuitable for language pairs with big differences\nin word order making the quality of translation\nfar from satisfactory. With the progress in deep\nlearning being applied to MT, in 2014, end-to-end\nneural network translation model was proposed\nby (Bahdanau et al., 2014; Sutskever et al., 2014)\nwhere the term \u201dneural machine translation\u201d wasformally used. Neural machine translation (NMT)\nis the newest method of MT that uses a single\nlarge neural network to model the entire transla-\ntion process, freeing the need for excessive fea-\nture engineering. Through the rapid research and\nbreakthroughs, end-to-end neural machine trans-\nlation has gained remarkable performances (Shi\net al., 2021; Bahdanau et al., 2014) and have be-\ncome mainstream approach to MT.\n2 Related Work\nEarly problem of NMT was often the poor trans-\nlation for long sentences (Sutskever et al., 2014)\nwhich can be attributed to the \ufb01xed-length of\nsource encoding in conventional encoder-decoder\nas suggested by Cho et al. (2014a) for which the\nconcept of attention to NMT was introduced by\nBahdanau et al. (2014) to avoid keeping a \ufb01xed\nsource side representation.\nAs compared to separately tuned components in\nSMT, newly emerging Neural Machine translation\nradically departures from previous machine learn-\ning approaches as the training of NMT is end-to-\nend which has signi\ufb01cantly improved translation\nquality across 30 different languages (Junczys-\nDowmunt et al., 2016). NMT model can be attrac-\ntive for various reason one being scalability issue,\nwhether it be memory requirements or computa-\ntional speed. Another being able to train all the\ncharacter embedding as each characters frequently\noccurs in the training corpus.\nMost neural machine translation models pro-\nposed use encoder-decoder where a neural net-\nwork reads and encodes a source sentence into\na \ufb01xed-length vector and a decoder then outputs\na translation from the encoded vector, where in\nmost of the cases the encoder and decoder are\nmainly implemented as RNNs, CNNs or self-\nattention network (Wu et al., 2018). The whole\nencoder\u2013decoder system, which consists of the\nencoder and the decoder for a language pair, is", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3682, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f1560dd1-dbe5-4104-b8b7-c732b7f904fe": {"__data__": {"id_": "f1560dd1-dbe5-4104-b8b7-c732b7f904fe", "embedding": null, "metadata": {"page_label": "3", "file_name": "MachineTranslationWithAttention.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\MachineTranslationWithAttention.pdf", "file_type": "application/pdf", "file_size": 481968, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8ecdbeb9-8a12-4d93-81eb-d9f4146f377e", "node_type": "4", "metadata": {"page_label": "3", "file_name": "MachineTranslationWithAttention.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\MachineTranslationWithAttention.pdf", "file_type": "application/pdf", "file_size": 481968, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "12eecc5dbf70bb045c9f3aa6356f9eae3448d57d7c5127a971070180eee17ba5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1f4f2a6d-b42d-4ebc-99a0-c8aa18dbb9b3", "node_type": "1", "metadata": {}, "hash": "fa2a2333f7f38f3c5867b4bdb39605eb8dea573c161658c8a143370e455fca2e", "class_name": "RelatedNodeInfo"}}, "text": "jointly trained to maximize the probability of a\ncorrect translation given a source sentence (Bah-\ndanau et al., 2014). After the initial proposal by\n(Sutskever et al., 2014; Bahdanau et al., 2014),\nmuch work has been done on the sequence-to-\nsequence neural machine translation model rang-\ning from new attention mechanism (Luong et al.,\n2015) to working on the problem of out-of-\nvocabulary words (Jean et al., 2015) for which se-\nquential RNNs are used both for encoding source\nsentences and generating target translation.\nWe draw our inspiration for machine transla-\ntion with attention from Bahdanau et al. (2014).\nWe have chosen to base our project on this paper\nas attention mechanism has been widely used as\nbaseline and is thoroughly studied among the NLP\ncommunity.\n3 Model\nMachine Translation is equivalent to maximizing a\nconditional probability of a target sentence given\nthe source sentence. In Neural Machine Trans-\nlation, we parameterize it to maximise the condi-\ntional probability. The approach used by Cho et al.\n(2014b) was to encode the source sentence into\na \ufb01xed-length vector, which becomes dif\ufb01cult to\ncompress all the necessary information into a \ufb01xed\nlength vector, which makes it dif\ufb01cult for the Neu-\nral Network to handle long sentences, and thus the\nperformance of the encoder-decoder drops as the\nlength of the sentences increases. So we use the\nmodel proposed by Bahdanau et al. (2014), where\nit does not encode the input sentence into a \ufb01xed-\nlength vector, rather than it simply encodes the in-\nput sentence into sequence of vectors, and while\ndecoding the translation, it select subset of vectors\nfrom the using attention mechanism. And Bah-\ndanau et al. (2014) showed that encoder decoder\nmodel with attention mechanism cope better with\nlong sentences. In the next section, we will \ufb01rst\ngive a brief introduction on encoder-decoder, the\nRNN, and one of its type, GRU, the one we used\nin our model and \ufb01nally the attention mechanism.\n3.1 Encoder-Decoder\nEncoder-Decoder, \ufb01rst proposed by Cho et al.\n(2014b), basically consists of 2 parts, the encoder\nand the decoder. Encoder codes the sequence\nof input sentence into dense vector representa-\ntion, and then decoder takes in the encoded sen-\ntence and decode the representation into anothersequences of words. They are trained to maximize\nthe conditional probability of the output sentence,\ngiven the input sentence.\nRNN is necessary when we need to maintain the\nword order in a sentence. This is not handled by\nbags of words model or other statistical models. In\naddition to input, xiand output \u02c6yi, we also have a\nstate vector, ai, which is initialized with vectors of\nzeros.iwould be the ithtimestep In the \ufb01rst layer\nof RNN, the input and state vector is fed into the\nrecurrent unit, the recurent unit may look like1:\nat=f(Waaat\u22121+Waxxi),\nand,\n\u02c6yt=g(Wyaat)\nwhere, t is the time step, Waais the weights be-\ntween two hidden layer, Waxis the weight be-\ntween input and hidden layer, and Wyais the\nweight between hidden and the output layer, and\nfcan betanh orRelu activation function, and g\ncan besigmoid orsoftmax activation function.\nAfter feeding the input to rnn unit, it returns a new\nstate vector in the next time step. This new state\nvector will be mapped to the output vector using\nsome function. This output vector can be used as\na prediction. The new state vector is cached and is\npassed across the next unit of the RNN, along with\nthe input in order for it to return the next state vec-\ntor. This happens recursively for all the input ele-\nments. So, when the model is reading the second\nword, instead of just predicting output using only\nthe second word, it also gets information from the\nprevious time step (\ufb01rst word) in terms of the state\nvector.\nOne of the problem of RNN is that it runs into\nthe problem of Vaishing gradient, \ufb01rst described\nby Hochreiter (1998).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3868, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1f4f2a6d-b42d-4ebc-99a0-c8aa18dbb9b3": {"__data__": {"id_": "1f4f2a6d-b42d-4ebc-99a0-c8aa18dbb9b3", "embedding": null, "metadata": {"page_label": "3", "file_name": "MachineTranslationWithAttention.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\MachineTranslationWithAttention.pdf", "file_type": "application/pdf", "file_size": 481968, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8ecdbeb9-8a12-4d93-81eb-d9f4146f377e", "node_type": "4", "metadata": {"page_label": "3", "file_name": "MachineTranslationWithAttention.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\MachineTranslationWithAttention.pdf", "file_type": "application/pdf", "file_size": 481968, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "12eecc5dbf70bb045c9f3aa6356f9eae3448d57d7c5127a971070180eee17ba5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f1560dd1-dbe5-4104-b8b7-c732b7f904fe", "node_type": "1", "metadata": {"page_label": "3", "file_name": "MachineTranslationWithAttention.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\MachineTranslationWithAttention.pdf", "file_type": "application/pdf", "file_size": 481968, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "a06e3c47e2cb7ae0665c691837e93512c8d93b419150ce52cbb3f1dd675513e0", "class_name": "RelatedNodeInfo"}}, "text": "This happens when we have\na very long sentence, which tends to have long\nterm dependencies. That means a word at the end\nof the sentence would be semantically dependent\non the word occurring at the beginning of the sen-\ntence. So, the gradients during the backpropaga-\ntion step would have a very hard time propagating\nback to affect the words or weights of the earlier\nunits. The gradients diminishes in the backpropa-\ngation step and not able to reach the earlier units.\nGenerally, RNN has local in\ufb02uences where a word\nis mainly in\ufb02uenced by words closed to it. So that\nmakes it dif\ufb01cult for the output at the later unit\n1https://www.coursera.org/learn/nlp-sequence-\nmodels/home/welcome", "mimetype": "text/plain", "start_char_idx": 3869, "end_char_idx": 4557, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7952aaaf-cee5-45d1-9f43-5f2de505d4e0": {"__data__": {"id_": "7952aaaf-cee5-45d1-9f43-5f2de505d4e0", "embedding": null, "metadata": {"page_label": "4", "file_name": "MachineTranslationWithAttention.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\MachineTranslationWithAttention.pdf", "file_type": "application/pdf", "file_size": 481968, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "313c1bb9-a842-416a-b74f-d7ba5963f8b9", "node_type": "4", "metadata": {"page_label": "4", "file_name": "MachineTranslationWithAttention.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\MachineTranslationWithAttention.pdf", "file_type": "application/pdf", "file_size": 481968, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "742d84c4ba1600bc5b6118bf7fd04e312a9e3d18f8765c18b10a0772f688dc62", "class_name": "RelatedNodeInfo"}}, "text": "Figure 1: The left diagram represents Long Short Term Memory Unit, with ias an Input gate, oas an\nOutput Gate, and fas a Forget Gate. The right diagram represents Gated Recurrent Unit, with ras Reset\nGate, andzas an Update Gate. (Chung et al., 2014)\nto be strongly in\ufb02uenced by a word that was very\nearly in the sequence.\nBut we can further improve the training by us-\ning other RNN units, like GRU (Chung et al.,\n2014) or LSTM (Hochreiter and Schmidhuber,\n1997), which are better at capturing long-range de-\npendencies. The state vector in simple RNN can\nbe considered as a memory, where the memory ac-\ncess was not controlled. At each step, the entire\nmemory state was read and updated. But in GRU\nand LSTM, we use a gating mechanism to control\nthe memory. Since, we used GRU in our model,\nso we will only describe GRU here.\nIn GRU, (Chung et al., 2014; Rana, 2016), the\nactivationhj\ntis a linear interpolation between the\nprevious activation hj\nt\u22121and the candidate ativa-\ntion\u02dchj\nt:\nhj\nt= (1\u2212zj\nt)hj\nt\u22121+zj\nt\u02dchj\nt\nwhere,zj\ntis the update gate, that decides how\nmuch GRU units updates its activation [See Fig.\n1]. The update gate is given by :\nzj\nt=\u03c3(Wzxt+Uzht\u22121)j.\nAnd the candidate activation \u02c6hj\ntis computed by :\n\u02c6hj\nt=tanh (Wx t+U(rt\u2299ht\u22121))j,\nwhere\u2299denotes element-wise multiplication and\nrj\ntare reset gates. When a reset gate at speci\ufb01c\ntime, t is set to 0, i.e. rj\nt== 0 , which makes the\nGRU to forget the past, i.e. forget the previousstate vectors. This is considered same as reading\nthe \ufb01rst word of the input sentence. And \ufb01nally,\nwe can compute the reset gate by :\nrj\nt=\u03c3(Wrxt+Urht\u22121)j.\nOne of the weakness of RNN is that it only uses\ninformation that is earlier in the sequence to make\npredictions but not the information which are later\nin the sequence. When predicting the output at\ntime step i, it does not use the word at time step i+1\nor i+2 or any other words in the later time step. So,\nit would be useful to know not just the information\nfrom the words from the previous time step but\nalso the information from the later time steps.\nSo, we use Bidirectional RNN, \ufb01rst proposed\nby (Schuster and Paliwal, 1997). From a point\nin time, it takes information from both the ear-\nlier and later time step in the sequence. The\nFirst RNN, which we called forward RNN,\u2212 \u2192fis\nfed the input sequence as it is. And the second\nRNN, which is also called backward RNN,\u2190 \u2212f\nis fed the input sequence in reverse order. This\ngives two separate state vectors \u2013 a forward state\nvector,\u2212 \u2192hT\nj, and a backward state vector\u2190 \u2212hT\nj.\n\u2212 \u2192hT\njwould be a sequence of forward hidden state\nvectors, (\u2212 \u2192h1,...,\u2212 \u2192hTx), and similarly, backward\nstate vector\u2190 \u2212hT\njwould be a sequence of backward\nhidden state vector, (\u2190 \u2212h1,...,\u2190 \u2212hTx). And the out-\nput at a speci\ufb01c timestep is accounted by the con-\ncatenation of output of two RNN\u2019s, concatenating\u2212 \u2192hjand\u2190 \u2212hj, i.e.hj= [\u2212 \u2192hT\nj,\u2190 \u2212hT\nj]. So, when pre-\ndicting the output at a speci\ufb01c time step, it will", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2948, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e0c8dcbc-1938-462e-b499-ffc4e59f452d": {"__data__": {"id_": "e0c8dcbc-1938-462e-b499-ffc4e59f452d", "embedding": null, "metadata": {"page_label": "5", "file_name": "MachineTranslationWithAttention.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\MachineTranslationWithAttention.pdf", "file_type": "application/pdf", "file_size": 481968, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fc92ecc4-86aa-46fd-938a-eb82b83e7c8b", "node_type": "4", "metadata": {"page_label": "5", "file_name": "MachineTranslationWithAttention.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\MachineTranslationWithAttention.pdf", "file_type": "application/pdf", "file_size": 481968, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "4e57516d8f90affd0c4d2929306120c712b3cd94a0351ccf09955402bd0de268", "class_name": "RelatedNodeInfo"}}, "text": "Figure 2: Attention Model (Luong et al., 2015)\nuse the information from the past, present as well\nas from the future. We need the entire sequence of\ndata before we can make any predictions.\nThe architecture that we are proposing here is\nbased on the Encoder-Decoder Framework. The\nencoder takes in the input sentence and converts\nthem into a vector representation\nThe encoder can be an RNN (Cho et al., 2014b)\nor LSTM unit (Sutskever et al., 2014). They pro-\ncesses the input sentence, pass it through RNN or\nLSTM, and when it encounters the end of sen-\ntence, then the hidden state, that captured all the\nrelevant information passes it to the decoder. Then\nthis information is used to predict the translations\nin the decoder, which can be RNN or LSTM, until\nit predicts the end of the sentence token. The hid-\nden state needs to remember every word from the\ninput sentence. So that is why this model tends to\nwork for short sentences and not long sentences.\nEven though if we used LSTM or GRU, which\ntends to remember the words that occured very\nearly in the sequence, it will still not be able to\nlearn the alignment between the source word and\nthe target word. They often forget the initial part\nof the sentence once they are processed in the en-\ncoder. That is why we use an alignment mecha-\nnism called attention. They help to memorize this\ninformation for longer sentences. But we need to\nlearn this alignment. It can vary from language to\nlanguage.3.2 Attention\nIn this section, we will speci\ufb01cally de\ufb01ne the\nalignment mechanism, [See Figure 2] that we used\nin our model. In RNN Encoder-Decoder model\n(Sutskever et al., 2014), we faced with the bottle-\nneck problem, where the complete sequence of in-\nformation of the source sentence, must be captured\nby one single vector, i.e. the last hidden unit of the\nencoder RNN is used as a context vector for the\ndecoder, which becomes dif\ufb01cult for the decoder\nto summarise large input sequence at once. This\nalso poses a problem where the encoder is not able\nto memorize the words coming at the beginning\nof the sentences, which leads to poor translation\nof the source sentence. The Attention mechanism\njust addresses this issue, by retaining and utilising\nall the hidden state of the input sentence during the\ndecoding phase.\nDuring the decoding phase, the model creates\nan alignment between each time step of the de-\ncoder output and all of the encoder hidden state.\nWe need to learn this alignment. Each output of\nthe decoder can selectively pick out speci\ufb01c ele-\nments from the sequence to produce the output.\nSo, this allows the model to focus and pay more\n\u201dAttention\u201d to the relevant part of the input se-\nquence.\nThe \ufb01rst attention model was proposed by Bah-\ndanau et al. (2014), there are several other types of\nattention proposed, such as the one by Luong et al.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2822, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2ee954f8-f718-482d-9b12-1e67742f741b": {"__data__": {"id_": "2ee954f8-f718-482d-9b12-1e67742f741b", "embedding": null, "metadata": {"page_label": "6", "file_name": "MachineTranslationWithAttention.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\MachineTranslationWithAttention.pdf", "file_type": "application/pdf", "file_size": 481968, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d67511a4-54ba-44c6-a886-b59f561c5ae5", "node_type": "4", "metadata": {"page_label": "6", "file_name": "MachineTranslationWithAttention.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\MachineTranslationWithAttention.pdf", "file_type": "application/pdf", "file_size": 481968, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "597d9547c6088bea14b48d158b5b34b516a73f849fa9bdcf77d90c4cae86c021", "class_name": "RelatedNodeInfo"}}, "text": "Spanish en la estrategia 2020 , reconocimos el hecho de que , si queremos mantener nuestro\nnivel de prosperidad en europa , tenemos que aumentar nuestra productividad .\nEnglish in the 2020 strategy , we acknowledged the fact that , if we are to maintain our level\nof prosperity in europe , we need to increase our productivity .\nSpanish sin embargo , es algo que debemos hacer si queremos demostrar a los estados unidos\nque nos deben considerar como un socio serio en la alianza contra el terrorismo .\nEnglish yet do it we must , if we are to demonstrate to the usa that we are to be taken\nseriously as a partner in an alliance against terrorism .\nSpanish sabr\u00b4an ustedes que fue tambi \u00b4en a instancias de esta c \u00b4amara que la comisi \u00b4on entabl \u00b4o\nnegociaciones , y estas han dado un resultado encomiable .\nEnglish you will be aware that it was not least at the insistence of this house that the com-\nmission entered into negotiations , and these have produced a creditable result .\nTable 1: Examples of Spanish and English sentences from the dataset\n(2015).\nWe will only discuss the attention model, pro-\nposed by Bahdanau et al. (2014). After the in-\nput sequence is passed through the encoder, it pro-\nduces hidden state for each of the elements in the\nsequence (h1,...,h Tx). Then we multiply the de-\ncoders hidden state at time step t (s1,...,s Ty),\nwith all of the encoders hidden state, which gives\nus the alignment score of each of the encoder out-\nput with respect to the decoder input and hidden\nstate at that time step:\net= [sT\nth1,...,sT\nthTx]\nThe alignment score quanti\ufb01es the amount of\nAttention the decoder will place on each of the en-\ncoder outputs when producing the next output, so\ninstead of looking at the entire sequence, it just\nconcentrate on few relevant parts of the sequence\nwhen predicting the next word.\nAfter calculating the alignment score, we pass\nthe vectoretthrough the softmax layer, to calcu-\nlate the probability distribution.\n\u03b1t=softmax (et)\nThen we multiply each of the attention weights\nwith each of the encoder hidden state, to get con-\ntext vector,at\nat=Tx\u2211\ni=1\u03b1t\nihi\nIf the attention score of speci\ufb01c element of the\ninput sequence is close to 1, then its in\ufb02uence on\nthe decoder output at that speci\ufb01c time step in-\ncreases. And then \ufb01nally, the context vector atproduced will be concatenated with the decoder\nhidden state, st, i.e.\n\u02dcht= [at,st]\nand is fed into decoder RNN, which produces new\nhidden state.\n4 Data\nWe used a Parallel Corpus for English-Spanish\nlanguage, which was extracted from the proceed-\nings of the European Parliament, also called Eu-\nroparl dataset (Koehn, 2005). It contains 1.96\nMillion sentences, each for English and Spanish.\nMost common words and count for both languages\nare shown below:\nWords Count\nde 1799827\n, 1456229\nla 1222089\nque 992176\n. 867284\nen 790382\nel 696521\ny 692640\na 577052\nlos 548495\nTable 2: SpanishWords Count\nthe 1956558\n, 1371506\nof 932044\nto 875415\n. 864674\nand 747108\nin 622426\nthat 476250\na 430093\nis 401782\nTable 3: English\nFirst we \ufb01lter out all the sentences having words\ngreater than 50. Then we sort these sentences\nbased on the number of words in each of the sen-\ntences, so that we have less padded sentences in\nour initial indices and sentences with high padding\nto be at the end of our indices, following with", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3315, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2433c747-6d6b-4d2f-9410-8fd181b41c7a": {"__data__": {"id_": "2433c747-6d6b-4d2f-9410-8fd181b41c7a", "embedding": null, "metadata": {"page_label": "7", "file_name": "MachineTranslationWithAttention.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\MachineTranslationWithAttention.pdf", "file_type": "application/pdf", "file_size": 481968, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f879b90f-dfbd-4773-9ece-db88ae9d007f", "node_type": "4", "metadata": {"page_label": "7", "file_name": "MachineTranslationWithAttention.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\MachineTranslationWithAttention.pdf", "file_type": "application/pdf", "file_size": 481968, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "b82c8f3d329ad7fcdcd9aa9278bed2613044583774863e83f9ee91399bf2d018", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6cf5fbd3-9c78-4bf6-94e9-c74b58ff9a19", "node_type": "1", "metadata": {}, "hash": "ebc4af17457a3a593b5e396bc6d74f00696d3da649f49548f6d14db70ed5fcd1", "class_name": "RelatedNodeInfo"}}, "text": "usual tokenization methods. Only preprocessing\nwe used was to lower case the words.\nWe selected 1 million sentences for the model-\ning due to the hardware constraints. We split the\ndataset into usual format, i.e. train, test and vali-\ndation. We used 900K sentences for our training,\n80K for validation and remaining 20K sentences\nfor the test set, which was not seen by the model\nduring training. We did not limit the vocabulary\nsize to any hard coded number, i.e. to get top N\nmost frequent words. The vocabulary size we got\nfrom English sentences was 36838 and for Span-\nish was 63220. Only token we added was End of\nSentence and Start of Sentence Tokens. We used\nSpanish as a source sentence and English as a tar-\nget sentence. See Table 1.\n5 Experiments\nThe encoder and decoder of our model have 256\nhidden units each. The encoder consist of for-\nward and backward gated recurrent unit (GRU)\neach having 256 hidden units. The decoder has\na single forward gated recurrent unit (GRU), with\n256 hidden dimensions, unlike 1000 hidden units,\nas in Bahdanau et al. (2014) due to the hardware\nlimitations. And we only trained the model for\nSpanish to English translation.\nWe used Adam optimizer to train the model, and\ngradient update is computed with a batch size of\n32 sentences.\nWe initialized our weights with xavier (Glo-\nrot) initializations, (Glorot and Bengio, 2010) with\nUniform Distribution, U[\u2212a,a], where\na=\u221a\n6\nnin+nout\nwhereninis the number of input neurons in the\nweight tensor, and noutis the number of output\nneurons in the weight tensor.\nThe total number of trainable parameters were\n43,564,519. We trained the model for roughly 20\nhours. After our model was trained, we use greedy\nsearch to predict the translation for the given input\nsentence, that maximizes the conditional proba-\nbility instead of using beam search as mentioned\nin the paper Bahdanau et al. (2014) due to our\nunfamiliarity and technical dif\ufb01culty dealing with\nBeam search.\nWe, then used BLEU (Papineni et al., 2002)\nscore to evaluate how the model was working on\nthe test data.6 Results\n6.1 Quantitative Results\nWe trained our model on maximum length of 50\nsentences, so any length smaller than or equal to\n50 is used for the training, We trained our model\nuntil the error on the validation data or the devel-\nopment data stops decreasing, in order to avoid\nthe problem of over\ufb01tting. We achieved a BLUE\nscore of 25.76on the test data and the error rate for\nSpanish to English translation was 4.267 on our\ntest data. According to Bahdanau et al. (2014), we\ncan say that our model out perform the encoder\ndecoder model, proposed by Cho et al. (2014b),\nfor 50 sentences, where they got BLEU score of\n17.82. Their performance drops when the length\nof the sentence is increased (Cho et al., 2014b).\nSo, the limitation of using \ufb01xed length vector\nin simple encoder decoder model in Cho et al.\n(2014b) work was the reason that it was under per-\nforming with long sentences.\nThis was our motivation to use the proposed ap-\nproach by Bahdanau et al. (2014), where the per-\nformance of the encoder-decoder with attention\nshows no deterioration with sentence of length\ngreater than 50 sentences. The result that we\ngot which was 25.76 was quite close to the Bah-\ndanau et al. (2014), where they got BLEU score of\n26.75,training with 1000 encoder and decoder di-\nmensions, and training on corpus of 384M words.\nThey were also able to achieve BLEU score of\n28.45 when trained the data until the performance\nof the validation stopped improving.\n6.2 Qualitative Results\nThe model proposed by Bahdanau et al. (2014)\nprovides a way to investigate the soft alignment\nbetween the translated sentence from the model\nand the input sentence.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3706, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6cf5fbd3-9c78-4bf6-94e9-c74b58ff9a19": {"__data__": {"id_": "6cf5fbd3-9c78-4bf6-94e9-c74b58ff9a19", "embedding": null, "metadata": {"page_label": "7", "file_name": "MachineTranslationWithAttention.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\MachineTranslationWithAttention.pdf", "file_type": "application/pdf", "file_size": 481968, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f879b90f-dfbd-4773-9ece-db88ae9d007f", "node_type": "4", "metadata": {"page_label": "7", "file_name": "MachineTranslationWithAttention.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\MachineTranslationWithAttention.pdf", "file_type": "application/pdf", "file_size": 481968, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "b82c8f3d329ad7fcdcd9aa9278bed2613044583774863e83f9ee91399bf2d018", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2433c747-6d6b-4d2f-9410-8fd181b41c7a", "node_type": "1", "metadata": {"page_label": "7", "file_name": "MachineTranslationWithAttention.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\MachineTranslationWithAttention.pdf", "file_type": "application/pdf", "file_size": 481968, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "b828b25d4cb37cfeb7c8e9234a48dcefb6d5b43641585504506bf6ae1b55dfb6", "class_name": "RelatedNodeInfo"}}, "text": "The matrix given in Fig\n3, each of the cells represent the weights \u03b1ijof\nthe annotation of the j-th source word for the i-\nth target word. This helps in visualizing and see\nwhich word from the input sentence were con-\nsidered more important for generating the target\nword.\nWe see that majority of the weights are con-\ncentrated on the diagonal matrix, along with non-\nmonotonic alignments. The non-monotonic align-\nments would be high for long sentences, since the\nwords in long target sentence tends to have depen-\ndence on more than one word in source sentence.", "mimetype": "text/plain", "start_char_idx": 3707, "end_char_idx": 4270, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4611a36c-1649-4040-b41f-3185f544ee81": {"__data__": {"id_": "4611a36c-1649-4040-b41f-3185f544ee81", "embedding": null, "metadata": {"page_label": "8", "file_name": "MachineTranslationWithAttention.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\MachineTranslationWithAttention.pdf", "file_type": "application/pdf", "file_size": 481968, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "098cdf05-b928-44fb-8c61-ccc493ccce90", "node_type": "4", "metadata": {"page_label": "8", "file_name": "MachineTranslationWithAttention.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\MachineTranslationWithAttention.pdf", "file_type": "application/pdf", "file_size": 481968, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "c15b9eff26ff2cc30108ec1aaebfd37729273251578dadf1af18ae73c3b94c6e", "class_name": "RelatedNodeInfo"}}, "text": "(a)\n (b)\n(c)\n (d)\nFigure 3: Alignments translated from Spanish to English by our model. The row represents the translated\nsentence, English and the column represents the source sentence, Spanish. Each of the cells of the matrix\nrepresents weights, \u03b1ij, of the annotation of the j-th source word for the i-th translated word. (1:White,\n0:Black)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 343, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c75190e4-1b06-4eb0-a4b4-92da6269effb": {"__data__": {"id_": "c75190e4-1b06-4eb0-a4b4-92da6269effb", "embedding": null, "metadata": {"page_label": "9", "file_name": "MachineTranslationWithAttention.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\MachineTranslationWithAttention.pdf", "file_type": "application/pdf", "file_size": 481968, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "98b7239c-c3f6-474c-a5b1-86bdb0f9b7e5", "node_type": "4", "metadata": {"page_label": "9", "file_name": "MachineTranslationWithAttention.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\MachineTranslationWithAttention.pdf", "file_type": "application/pdf", "file_size": 481968, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "7f0787ce690edfe3fa1dc75a11f796aa01a67c5e4889f8438057327a8adadf48", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "22864b2f-6bba-4aa3-9dcf-45d5c130efc9", "node_type": "1", "metadata": {}, "hash": "170e7f5fa43fe3751114680f3ba56b5e88ea9ddbaf52650f64d05221b2b843a7", "class_name": "RelatedNodeInfo"}}, "text": "Let us take an example from the test set, con-\nsider the source sentence:\nson los estados miembros de la zona del\neuro los que no han cumplido , en espe-\ncialla republica federal de alemania que\nse niega a mantener su promesa.\nAnd its translation by our model is :\nthat it is the member states of the euro\narea which have not complied honour,\nparticularly the federal republic of ger-\nmany that refuses to keep their promise\nto sustain their promise.\nAnd the reference is:\nit is the member states of the euro area\nthat have not delivered - and particu-\nlarly the federal republic of germany,\nwhich is refusing to keep its promise.\nWe can observe from our translated sentence,\nthe model generates \u201dhave not complied \u201d , instead\nof\u201dhave not delivered\u201d (from reference sentence),\nwhich has the same meaning. It tries to preserve\nthe meaning of the whole sentence, and it does not\nblindly takes the word from the reference, it tries\nto generalize.\nRefer to Table 4 at the end of the paper for\nmore translations from Spanish to English using\nEncoder Decoder model with Attention Mecha-\nnism.\n7 Discussion\nAfter trying to achieve the result similar to what\nwas presented in the paper, we are satis\ufb01ed with\nour result though there is much that can be im-\nproved. Limitation caused by the hardware held\nus back from achieving better results. We used the\nserver provided by the university and as a backup\nused google colab for our work, so we had to be\nwary of the maintenance schedule happening in\nthe server and the limitation of 24 hrs of work-\ntime on google colab, which otherwise could in-\nterrupt while we were training our model. So,\nto overcome these challenges we decided to use\n1M sentences from each form Spanish and English\ndataset, and reduce the parameters for encoder-\ndecoder.\nWe also faced problem with length of the vo-\ncabulary size of English and Spanish sentences,\nwhere the vocabulary size of the English sen-\ntence were the output dimension, and vocabularysize of Spanish sentence were input dimensions of\nour model. This leads to increase in number of\ntrainable parameters, the decoding complexity in-\ncreases with number of target words (vocabulary\nsize of the English), where this problem has been\naddressed by Jean et al. (2015).\n8 Conclusion\nThe approach proposed by Cho et al. (2014b) was\nto encode the whole sentence into \ufb01xed length vec-\ntor, and this becomes problematic when dealing\nwith long sentences. We extend this basic encoder-\ndecoder model by an Attention mechanism (Bah-\ndanau et al., 2014), where the model searches for\nthe input word computed from the encoder, which\nbest align with the target word, when generat-\ning each target word. This frees the model from\nhaving to encode the source sentence into a \ufb01xed\nlength vector, only rely on the information rele-\nvant for generating each target word. We com-\npared our model for Spanish to English translation\nwith both of these approaches, and found that our\nmodel works better than the encoder-decoder ap-\nproach (Cho et al., 2014b), and have slightly lower\nresults than the architecture with Attention mech-\nanism, (Bahdanau et al., 2014). We also observed\nthat the model tries to align the target word with\nthe relevant word from the translated sentence.\nIn the future work, there are several things we\ncan try. We can train our model on much larger\ndataset, with a better hardware and with differ-\nent attention models. We can also focus on how\nto handle the stop words, punctuation, as we can\nsee on Table 2 and Table 3, which accounts for\nthe highest word count, and also to handle the un-\nknown words, which does not appear in the train-\ning data, but in the test data.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua\nBengio. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint\narXiv:1409.0473 , 2014.\nKyunghyun Cho, B. V . Merrienboer, Dzmitry\nBahdanau, and Yoshua Bengio. On the\nproperties of neural machine translation: En-\ncoder\u2013decoder approaches. In SSST@EMNLP ,\n2014a.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4008, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "22864b2f-6bba-4aa3-9dcf-45d5c130efc9": {"__data__": {"id_": "22864b2f-6bba-4aa3-9dcf-45d5c130efc9", "embedding": null, "metadata": {"page_label": "9", "file_name": "MachineTranslationWithAttention.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\MachineTranslationWithAttention.pdf", "file_type": "application/pdf", "file_size": 481968, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "98b7239c-c3f6-474c-a5b1-86bdb0f9b7e5", "node_type": "4", "metadata": {"page_label": "9", "file_name": "MachineTranslationWithAttention.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\MachineTranslationWithAttention.pdf", "file_type": "application/pdf", "file_size": 481968, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "7f0787ce690edfe3fa1dc75a11f796aa01a67c5e4889f8438057327a8adadf48", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c75190e4-1b06-4eb0-a4b4-92da6269effb", "node_type": "1", "metadata": {"page_label": "9", "file_name": "MachineTranslationWithAttention.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\MachineTranslationWithAttention.pdf", "file_type": "application/pdf", "file_size": 481968, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "337731470a27b7d240913210b5f11c53e952994c0009ef71479288396cfa016a", "class_name": "RelatedNodeInfo"}}, "text": "In SSST@EMNLP ,\n2014a.\nKyunghyun Cho, B. V . Merrienboer, C \u00b8 aglar\nG\u00a8ulc \u00b8ehre, Dzmitry Bahdanau, Fethi Bougares,", "mimetype": "text/plain", "start_char_idx": 3986, "end_char_idx": 4100, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "09998943-fff4-47ff-a5ac-f8d80ab86268": {"__data__": {"id_": "09998943-fff4-47ff-a5ac-f8d80ab86268", "embedding": null, "metadata": {"page_label": "10", "file_name": "MachineTranslationWithAttention.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\MachineTranslationWithAttention.pdf", "file_type": "application/pdf", "file_size": 481968, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "95c5161b-01a6-4cd3-a36e-adac2180cd0b", "node_type": "4", "metadata": {"page_label": "10", "file_name": "MachineTranslationWithAttention.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\MachineTranslationWithAttention.pdf", "file_type": "application/pdf", "file_size": 481968, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "c5e15ba46dc63c7e7b745af2553d2757fd34a7eaeadd04485caa77346cc94667", "class_name": "RelatedNodeInfo"}}, "text": "Holger Schwenk, and Yoshua Bengio. Learn-\ning phrase representations using rnn en-\ncoder\u2013decoder for statistical machine transla-\ntion. In EMNLP , 2014b.\nJ. Chung, C \u00b8 aglar G \u00a8ulc \u00b8ehre, Kyunghyun Cho, and\nYoshua Bengio. Empirical evaluation of gated\nrecurrent neural networks on sequence model-\ning.ArXiv , abs/1412.3555, 2014.\nXavier Glorot and Yoshua Bengio. Understanding\nthe dif\ufb01culty of training deep feedforward neu-\nral networks. In AISTATS , 2010.\nS. Hochreiter. The vanishing gradient problem\nduring learning recurrent neural nets and prob-\nlem solutions. Int. J. Uncertain. Fuzziness\nKnowl. Based Syst. , 6:107\u2013116, 1998.\nS. Hochreiter and J. Schmidhuber. Long short-\nterm memory. Neural Computation , 9:1735\u2013\n1780, 1997.\nS\u00b4ebastien Jean, Kyunghyun Cho, R. Memisevic,\nand Yoshua Bengio. On using very large tar-\nget vocabulary for neural machine translation.\nArXiv , abs/1412.2007, 2015.\nMarcin Junczys-Dowmunt, Tomasz Dwojak, and\nHieu Hoang. Is neural machine translation\nready for deployment? a case study on 30 trans-\nlation directions. 01 2016.\nPhilipp Koehn. Europarl: A parallel corpus for\nstatistical machine translation. In MTSUMMIT ,\n2005.\nPhilipp Koehn, Franz Josef Och, and Daniel\nMarcu. Statistical phrase-based translation.\nInProceedings of the 2003 Conference of the\nNorth American Chapter of the Association\nfor Computational Linguistics on Human Lan-\nguage Technology - Volume 1 , NAACL \u201903,\npage 48\u201354, USA, 2003. Association for Com-\nputational Linguistics. doi: 10.3115/1073445.\n1073462. URL https://doi.org/10.\n3115/1073445.1073462 .\nThang Luong, Hieu Pham, and Christopher D.\nManning. Effective approaches to attention-\nbased neural machine translation. In EMNLP ,\n2015.\nKishore Papineni, S. Roukos, T. Ward, and Wei-\nJing Zhu. Bleu: a method for automatic evalua-\ntion of machine translation. In ACL, 2002.\nR. Rana. Gated recurrent unit (gru) for emo-\ntion classi\ufb01cation from noisy speech. ArXiv ,\nabs/1612.07778, 2016.M. Schuster and K. Paliwal. Bidirectional recur-\nrent neural networks. IEEE Trans. Signal Pro-\ncess. , 45:2673\u20132681, 1997.\nXuewen Shi, Heyan Huang, Ping Jian, and Yi-Kun\nTang. Improving neural machine translation\nwith sentence alignment learning. Neurocom-\nputing , 420:15\u201326, 2021. ISSN 0925-2312. doi:\nhttps://doi.org/10.1016/j.neucom.2020.05.104.\nIlya Sutskever, Oriol Vinyals, and Quoc V . Le.\nSequence to sequence learning with neural net-\nworks. In NIPS , 2014.\nShuangzhi Wu, Dongdong Zhang, Zhirui Zhang,\nNan Yang, Mu Li, and M. Zhou. Dependency-\nto-dependency neural machine translation.\nIEEE/ACM Transactions on Audio, Speech, and\nLanguage Processing , 26:2132\u20132141, 2018.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2634, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e0d654c5-7220-45b7-b66f-5ffd6a4ef596": {"__data__": {"id_": "e0d654c5-7220-45b7-b66f-5ffd6a4ef596", "embedding": null, "metadata": {"page_label": "11", "file_name": "MachineTranslationWithAttention.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\MachineTranslationWithAttention.pdf", "file_type": "application/pdf", "file_size": 481968, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9e103825-8811-48ec-8070-e9b5f6ce841d", "node_type": "4", "metadata": {"page_label": "11", "file_name": "MachineTranslationWithAttention.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\MachineTranslationWithAttention.pdf", "file_type": "application/pdf", "file_size": 481968, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}, "hash": "a9f22e082e8c226b38909d21ae8d22642aeca1a025fdc5aac026fd1f4cc453ea", "class_name": "RelatedNodeInfo"}}, "text": "Source se\u02dcnor presidente , se \u02dcnor presidente en ejercicio del consejo , se \u02dcnor presidente de la\ncomisi \u00b4on , se \u02dcnor\u00b4\u0131as , me gustar \u00b4\u0131a hacer tres breves comentarios .\nReference mr president , mr president - in - of\ufb01ce of the council , mr president of the com-\nmission , ladies and gentlemen , i would just like to make three brief comments\n.\nOur Model president mr president , mr president - in - of\ufb01ce of the council , mr president of\nthe commission , ladies and gentlemen , i would like to make three brief comments\nbrief comments .\nGoogle Translate Mr. Chairman, Mr. Chairman-in-Of\ufb01ce of the Council, Mr. Chairman of the Com-\nmittee, ladies and gentlemen, I would like to make three brief comments.\nSource como los estados miembros , la comisi \u00b4on procura promover el estado de derecho ,\nsin el cual los derechos humanos no obtendr \u00b4an reconocimiento en ning \u00b4un territorio\n.\nReference the commission is involved , as are member states , in the promotion of the rule of\nlaw , without which human rights can not operate in any territory .\nOur Model that as the member states , the commission intends to promote the rule of law ,\nwithout human rights will not not be any recognition in any territory in any territory\n.\nGoogle Translate Like the member states, the commission seeks to promote the rule of law, without\nwhich human rights will not be recognized in any territory.\nSource por escrito . - he votado a favor del informe de la se \u02dcnora fraga , que permite a\ngroenlandia exportar productos pesqueros a la ue a pesar de no ser miembro .\nReference in writing . - i voted in favour of ms fraga \u2019s report , which allows greenland to\nexport \ufb01shery products to the eu despite not being a member .\nOur Model in writing . - i voted in favour of mrs fraga est \u00b4evez \u2019s report , which allows greenland\nexport export to export to the eu despite despite not being member .\nGoogle Translate written . - i voted in favor of the report by mrs fraga, which allows greenland to\nexport \ufb01shery products to the eu despite not being a member.\nSource ( pl ) se \u02dcnor presidente , me gustar \u00b4\u0131a una vez m \u00b4as manifestar mi satisfacci \u00b4on por la\nimportancia que la comunidad con\ufb01ere a la necesidad de innovaci \u00b4on en europa .\nReference ( pl ) mr president , i would like once again to express my pleasure at the importance\nthat the community attaches to the need for innovation in europe .\nOur Model that ( pl ) mr president , i would once again like to express my satisfaction satis-\nfaction that the community attaches to the community to the need for innovation in\neurope .\nGoogle Translate (pl) mr president, i would like once again to express my satisfaction with the impor-\ntance that the community attaches to the need for innovation in europe.\nSource son los estados miembros de la zona del euro los que no han cumplido , en especial\nla rep \u00b4ublica federal de alemania que se niega a mantener su promesa .\nReference it is the member states of the euro area that have not delivered - and particularly the\nfederal republic of germany , which is refusing to keep its promise .\nOur Model that it is the member states of the euro area which have not complied honour ,\nparticularly the federal republic of germany that refuses to keep their promise to\nsustain their promise .\nGoogle Translate it is the eurozone member states that have not delivered, especially the federal re-\npublic of germany which refuses to keep its promise.\nTable 4: Source and Reference form the test data, with translated sentence from our model along with\nGoogle translation (as of 16 August 2021)\nView publication stats", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3586, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"495681bc-d51f-4308-b31a-84caa8468a3b": {"node_ids": ["3c690256-4434-442d-bb66-307798698755", "06930175-2152-4a27-b699-a0244fcb66b7"], "metadata": {"page_label": "1", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}}, "a0dee89d-f766-415a-84eb-2ed0212c6e4e": {"node_ids": ["3a6394d7-c53a-482f-a085-fe633c55c805"], "metadata": {"page_label": "2", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}}, "b6e267d8-1c49-4c47-a166-2566b6ffc3b5": {"node_ids": ["6dcb0487-8c7f-4f26-9ca2-aa72ddd9bba1", "9f9cdd59-7a12-46d5-9215-e6a61d3c87c5", "4022fbee-91c7-4511-b72c-a311a8b732f3", "c3633805-5a5f-4762-b6a0-3e6c58f1bf94", "76847fe2-68b2-4d53-8dbb-72eed750f1dd"], "metadata": {"page_label": "3", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}}, "77bb7dc3-71c2-4862-9653-1c6a188f9589": {"node_ids": ["64d15b93-d8a9-4b51-8302-e377fa135abe", "82ea3b5a-aaf4-4b6f-971b-3b683badc115"], "metadata": {"page_label": "4", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}}, "5f687132-f180-41f6-81f4-73289528c737": {"node_ids": ["21812bc7-cdc8-428f-96be-c54e3263dc38", "db73b87d-b7ed-44a4-afca-4738c55a929d"], "metadata": {"page_label": "5", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}}, "4e5849c6-12e9-4373-9da0-5decbb86017e": {"node_ids": ["760f5448-22ac-477b-ad23-f88ed331c0f3", "0b0ebff4-a6fb-4d3f-a597-2fa254cc5360"], "metadata": {"page_label": "6", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}}, "e71b2695-f5dc-4ee1-a004-eb1e2fd0a09c": {"node_ids": ["0a581623-32a7-4b0a-af17-10401dae3dc7", "a31faf5a-76a0-48dc-86b1-1dada8caf085"], "metadata": {"page_label": "7", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}}, "0299c721-d0c8-4bae-a221-b8008e112839": {"node_ids": ["71017e2f-415d-4348-b708-8b0e34a8dfe5", "91e2a0ab-3657-4260-8cbc-34b0e827b9fa"], "metadata": {"page_label": "8", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}}, "ee28887d-94b6-4892-b0f8-0d3ce82210ab": {"node_ids": ["dc71cc6f-3115-4644-bfe5-acf79adb2a26", "b3ba7b87-ccaa-4e9e-93e4-7a56ce10ebcf"], "metadata": {"page_label": "9", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}}, "42fd54e6-5473-4cbf-9d08-90c03c85bbde": {"node_ids": ["f1312f1e-7f6e-4e0e-be92-f3c3ccb7a014", "3fdff81a-7742-4405-a889-a3b820277638"], "metadata": {"page_label": "10", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}}, "a275cc4e-a022-498c-a869-6013536c367f": {"node_ids": ["f54ff78d-a4a3-4460-a49a-41f88a058079", "003a9261-5643-4655-afab-e25cb6260094"], "metadata": {"page_label": "11", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}}, "d5dfac96-c94e-48b7-927d-ace75146706f": {"node_ids": ["3948973c-04a3-4443-a96d-44b243f042e1", "48de820b-9318-4dcc-91db-f0a6c9f8847b"], "metadata": {"page_label": "12", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}}, "8d94c55a-8566-4f87-a584-9786cc35f753": {"node_ids": ["4304dee3-e26d-4278-a690-a35b8e99d5d3"], "metadata": {"page_label": "13", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}}, "f40f0e73-aa49-467f-b26e-1b77ae3a1f79": {"node_ids": ["6f0fe2cc-ca36-493b-bd8e-ea9b218518bd"], "metadata": {"page_label": "14", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}}, "2ac0cc0d-5faa-4e16-82a4-1f3a303d7006": {"node_ids": ["d0581db3-4277-4801-920d-47f32fff28f7"], "metadata": {"page_label": "15", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}}, "a31fcd72-9352-4e2a-a6ff-4a35fa85a8bf": {"node_ids": ["bd207ab8-21a7-4063-b9f1-7b3e98043d33"], "metadata": {"page_label": "16", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}}, "a9025ae8-c380-403f-8f01-abf432feb81e": {"node_ids": ["300cb555-aab8-40dc-b8f6-c467e2e905f9"], "metadata": {"page_label": "17", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}}, "c48cb58e-c7c4-45aa-8210-a877662d6ec5": {"node_ids": ["b11f282c-5ead-4150-b193-9ce037de8ff3"], "metadata": {"page_label": "18", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}}, "e2f11d09-4e6e-4ee8-aa84-500728571ef9": {"node_ids": ["6f00e998-e4a7-4a34-978e-421358819c3a"], "metadata": {"page_label": "19", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}}, "f735b33d-be49-47f2-a8e4-614d2963e5d5": {"node_ids": ["518693d7-8057-4a6c-9771-e1a97399f2da"], "metadata": {"page_label": "20", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}}, "b37bbca1-ad20-4b10-a125-5703360ec665": {"node_ids": ["4dba1649-ec15-489e-9214-d51da906145e"], "metadata": {"page_label": "21", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}}, "f6bd37e8-19bd-4616-981e-541d875446b0": {"node_ids": ["93d8512f-5257-452f-bce9-5eea5f8c894d"], "metadata": {"page_label": "22", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}}, "d30725b2-8917-4b80-aa26-b6ab1d5d4328": {"node_ids": ["3b08562d-bf8e-4707-b3aa-4a677729d21f"], "metadata": {"page_label": "23", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}}, "9366bf78-585a-487f-919f-aaa1e1c8a2d7": {"node_ids": ["63819359-9937-491d-a1b3-54cdae85df43"], "metadata": {"page_label": "24", "file_name": "DFA_RAG.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\DFA_RAG.pdf", "file_type": "application/pdf", "file_size": 4718961, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}}, "c85e4e9f-d83b-429e-925d-502f1e8647e4": {"node_ids": ["e6ae1976-9d75-4bad-aa02-3dffbb741229"], "metadata": {"page_label": "1", "file_name": "MachineTranslationWithAttention.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\MachineTranslationWithAttention.pdf", "file_type": "application/pdf", "file_size": 481968, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}}, "89a4565d-5b89-4b6a-9758-40eb6b17b1bb": {"node_ids": ["6157bec3-85a2-482b-9014-f4e118e443fa"], "metadata": {"page_label": "2", "file_name": "MachineTranslationWithAttention.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\MachineTranslationWithAttention.pdf", "file_type": "application/pdf", "file_size": 481968, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}}, "8ecdbeb9-8a12-4d93-81eb-d9f4146f377e": {"node_ids": ["f1560dd1-dbe5-4104-b8b7-c732b7f904fe", "1f4f2a6d-b42d-4ebc-99a0-c8aa18dbb9b3"], "metadata": {"page_label": "3", "file_name": "MachineTranslationWithAttention.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\MachineTranslationWithAttention.pdf", "file_type": "application/pdf", "file_size": 481968, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}}, "313c1bb9-a842-416a-b74f-d7ba5963f8b9": {"node_ids": ["7952aaaf-cee5-45d1-9f43-5f2de505d4e0"], "metadata": {"page_label": "4", "file_name": "MachineTranslationWithAttention.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\MachineTranslationWithAttention.pdf", "file_type": "application/pdf", "file_size": 481968, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}}, "fc92ecc4-86aa-46fd-938a-eb82b83e7c8b": {"node_ids": ["e0c8dcbc-1938-462e-b499-ffc4e59f452d"], "metadata": {"page_label": "5", "file_name": "MachineTranslationWithAttention.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\MachineTranslationWithAttention.pdf", "file_type": "application/pdf", "file_size": 481968, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}}, "d67511a4-54ba-44c6-a886-b59f561c5ae5": {"node_ids": ["2ee954f8-f718-482d-9b12-1e67742f741b"], "metadata": {"page_label": "6", "file_name": "MachineTranslationWithAttention.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\MachineTranslationWithAttention.pdf", "file_type": "application/pdf", "file_size": 481968, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}}, "f879b90f-dfbd-4773-9ece-db88ae9d007f": {"node_ids": ["2433c747-6d6b-4d2f-9410-8fd181b41c7a", "6cf5fbd3-9c78-4bf6-94e9-c74b58ff9a19"], "metadata": {"page_label": "7", "file_name": "MachineTranslationWithAttention.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\MachineTranslationWithAttention.pdf", "file_type": "application/pdf", "file_size": 481968, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}}, "098cdf05-b928-44fb-8c61-ccc493ccce90": {"node_ids": ["4611a36c-1649-4040-b41f-3185f544ee81"], "metadata": {"page_label": "8", "file_name": "MachineTranslationWithAttention.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\MachineTranslationWithAttention.pdf", "file_type": "application/pdf", "file_size": 481968, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}}, "98b7239c-c3f6-474c-a5b1-86bdb0f9b7e5": {"node_ids": ["c75190e4-1b06-4eb0-a4b4-92da6269effb", "22864b2f-6bba-4aa3-9dcf-45d5c130efc9"], "metadata": {"page_label": "9", "file_name": "MachineTranslationWithAttention.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\MachineTranslationWithAttention.pdf", "file_type": "application/pdf", "file_size": 481968, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}}, "95c5161b-01a6-4cd3-a36e-adac2180cd0b": {"node_ids": ["09998943-fff4-47ff-a5ac-f8d80ab86268"], "metadata": {"page_label": "10", "file_name": "MachineTranslationWithAttention.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\MachineTranslationWithAttention.pdf", "file_type": "application/pdf", "file_size": 481968, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}}, "9e103825-8811-48ec-8070-e9b5f6ce841d": {"node_ids": ["e0d654c5-7220-45b7-b66f-5ffd6a4ef596"], "metadata": {"page_label": "11", "file_name": "MachineTranslationWithAttention.pdf", "file_path": "C:\\Users\\priya\\qasystem\\temp\\MachineTranslationWithAttention.pdf", "file_type": "application/pdf", "file_size": 481968, "creation_date": "2024-09-23", "last_modified_date": "2024-09-23"}}}}